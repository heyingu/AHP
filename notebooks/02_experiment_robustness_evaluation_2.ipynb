{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae37e2c-c9a6-4c8a-9f0a-c261b1cb88c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在下载 'averaged_perceptron_tagger'...\n"
     ]
    }
   ],
   "source": [
    "# !nvidia-smi\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "# --- 解决 SSL 证书问题 (如果您的环境需要) ---\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "# -----------------------------------------\n",
    "\n",
    "# 1. 下载修复当前错误的 'averaged_perceptron_tagger' 包\n",
    "print(\"正在下载 'averaged_perceptron_tagger'...\")\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# 2. 为防止 TextFooler 报其他错误，我们一次性下载它所有可能用到的包\n",
    "print(\"\\n正在下载 TextFooler 其他常用数据包...\")\n",
    "nltk.download('punkt')      # 用于分词\n",
    "nltk.download('stopwords')  # 用于停用词过滤\n",
    "nltk.download('wordnet')    # 用于同义词查找\n",
    "nltk.download('omw-1.4')    # wordnet 的一个依赖\n",
    "\n",
    "print(\"\\n所有需要的 NLTK 数据包已下载到 /root/nltk_data 目录！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fd2c56c-d1d4-43c2-aed1-2f27165ce07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 补丁1生效：已成功注入“全功能”伪模块。\n",
      ">>> 补丁2生效：已成功替换 nltk.download 函数。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 15:18:46.493329: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-25 15:18:46.525486: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI AVX_VNNI_INT8 AVX_NE_CONVERT FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-25 15:18:47.170262: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 恭喜！环境设置最终完成，所有模块导入成功！---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/ahp_env/lib/python3.12/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# |      !!! 终极解决方案：“直捣黄龙”之最终极补丁 !!!             |\n",
    "# ====================================================================\n",
    "import sys\n",
    "import os\n",
    "from unittest.mock import patch\n",
    "import types\n",
    "from importlib.machinery import ModuleSpec\n",
    "\n",
    "# --- 补丁1：注入“全功能”伪模块 ---\n",
    "def create_full_fake_module(name, attributes_to_add):\n",
    "    spec = ModuleSpec(name, None)\n",
    "    module = types.ModuleType(name)\n",
    "    module.__spec__ = spec\n",
    "    for attr in attributes_to_add:\n",
    "        setattr(module, attr, lambda *args, **kwargs: None)\n",
    "    return module\n",
    "\n",
    "sys.modules['num2words'] = create_full_fake_module('num2words', ['num2words'])\n",
    "sys.modules['word2number'] = create_full_fake_module('word2number', ['w2n'])\n",
    "print(\">>> 补丁1生效：已成功注入“全功能”伪模块。\")\n",
    "\n",
    "# --- 补丁2：“直捣黄龙”，直接替换nltk.download函数 ---\n",
    "def dummy_nltk_download(*args, **kwargs):\n",
    "    print(\">>> 补丁2生效：已成功拦截并跳过 nltk.download() 调用！<<<\")\n",
    "    return True # 返回成功状态\n",
    "\n",
    "patcher = patch('nltk.download', dummy_nltk_download)\n",
    "patcher.start()\n",
    "print(\">>> 补丁2生效：已成功替换 nltk.download 函数。\")\n",
    "\n",
    "\n",
    "# --- 补丁3：手动为NLTK“指路” ---\n",
    "nltk_data_dir = os.path.expanduser('~/nltk_data')\n",
    "import nltk\n",
    "if nltk_data_dir not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_dir)\n",
    "    print(f\"成功将 '{nltk_data_dir}' 添加到NLTK的搜索路径。\")\n",
    "\n",
    "# --- 补丁4：设置其他环境变量 ---\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "sys.path.append('..')\n",
    "# ====================================================================\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- 正常导入所有模块 ---\n",
    "# 导入在 defenses.py 中定义的类\n",
    "from src.defenses import BasePredictor, NoDefense, AhpDefense, SelfDenoiseDefense\n",
    "from src.utils.data_loader import load_sst2_dataset\n",
    "from src.models.model_loader import load_main_llm\n",
    "from src.attacks import AttackerWrapper\n",
    "from src.utils.metrics import calculate_accuracy, calculate_asr\n",
    "\n",
    "print(\"\\n--- 恭喜！环境设置最终完成，所有模块导入成功！---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e2df9be-4938-4a90-82d7-a620824d4332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 实验设置 ---\n",
    "TASK = 'sst2'\n",
    "DATASET_NAME = 'SST-2'\n",
    "NUM_SAMPLES_TO_TEST = 50\n",
    "ATTACK_RECIPE = 'bae' # 可以切换为 'textbugger'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebbd5efc-dfd2-4ba8-b97c-2ad6ae929957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载主模型: /root/autodl-tmp/circulus_alpaca-7b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['pad_token_id']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86bdedd632d64be3a4cb68e84920777b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主模型以 4-bit 量化模式成功加载到GPU。\n",
      "正在加载SST-2数据集 (validation split)...\n",
      "SST-2数据集加载成功。\n"
     ]
    }
   ],
   "source": [
    "# --- 2. 加载模型和数据 ---\n",
    "# 请确保这里的路径是您正在使用的 alpaca-7b 模型的正确路径\n",
    "local_model_path = \"/root/autodl-tmp/alpaca-native\"\n",
    "# 使用4-bit量化加载模型，这是节省显存的第一道防线\n",
    "main_model, main_tokenizer = load_main_llm(model_name=local_model_path, use_4bit=True)\n",
    "# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# # 我们不再使用 load_main_llm，而是直接加载一个标准的分类模型\n",
    "# MODEL_NAME = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# print(f\"正在加载一个更小的、用于公平对比的基础模型: {MODEL_NAME}...\")\n",
    "\n",
    "# 加载模型和分词器\n",
    "# main_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "# main_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# main_model.to(\"cuda\") # 将模型移动到GPU\n",
    "\n",
    "dataset = load_sst2_dataset(split='validation').select(range(NUM_SAMPLES_TO_TEST))\n",
    "dataset_df = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db87408c-87c9-4ba6-a314-f1ef44daa8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基础模型加载并适配完成！\n",
      "正在加载NLI模型: roberta-large-mnli...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI模型加载成功。\n"
     ]
    }
   ],
   "source": [
    "# --- 3. 初始化防御策略和攻击器 ---\n",
    "# class SimplePredictor:\n",
    "#     def __init__(self, model, tokenizer):\n",
    "#         self.model = model\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.device = model.device\n",
    "\n",
    "#     def predict(self, sentence):\n",
    "#         inputs = self.tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
    "#         with torch.no_grad():\n",
    "#             logits = self.model(**inputs).logits\n",
    "        \n",
    "#         predicted_class_id = torch.argmax(logits, dim=1).item()\n",
    "#         # 对于 SST-2, 0 是 'negative', 1 是 'positive'\n",
    "#         return \"positive\" if predicted_class_id == 1 else \"negative\"\n",
    "# 创建基础预测器，所有防御和攻击都将基于它\n",
    "# 这里的 BasePredictor 来自于您提供的 defenses.py 文件\n",
    "base_predictor = BasePredictor(main_model, main_tokenizer, task=TASK)\n",
    "# base_predictor = SimplePredictor(main_model, main_tokenizer)\n",
    "\n",
    "print(\"基础模型加载并适配完成！\")\n",
    "\n",
    "# ===================== 关键修正：降低AHP防御的显存消耗 =====================\n",
    "# 将 m_val 从 10 大幅减少到 5。\n",
    "# 这是解决 CUDA_ERROR_INVALID_HANDLE (显存不足) 错误的核心步骤。\n",
    "# 因为 AHP 防御需要一次性生成 m_val 个候选句子，这个值过大会耗尽显存。\n",
    "AHP_M_VAL = 5 \n",
    "# =======================================================================\n",
    "\n",
    "# 定义要对比的防御策略\n",
    "defenses = {\n",
    "    \"No Defense (Baseline)\": NoDefense(base_predictor),\n",
    "    \"AHP-NLI Defense\": AhpDefense(base_predictor, k_val=3, m_val=AHP_M_VAL),\n",
    "    \"Self-Denoise Defense\": SelfDenoiseDefense(base_predictor, num_samples=10)\n",
    "}\n",
    "\n",
    "# 初始化攻击器\n",
    "# 这里的 AttackerWrapper 来自于您提供的 attacks/attacks.py 文件\n",
    "attacker = AttackerWrapper(base_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bade34d8-7710-46fb-978a-b66375909a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "textattack: Unknown if model of class <class 'src.attacks.attacks.ClassificationModelForAttack'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 成功移除存在兼容性问题的 UniversalSentenceEncoder 约束。\n",
      ">>> 已添加基于 PyTorch 的 WordEmbeddingDistance 约束作为替代。\n",
      "正在使用 bae 生成对抗样本...\n",
      "Attack(\n",
      "  (search_method): GreedyWordSwapWIR(\n",
      "    (wir_method):  unk\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapMaskedLM(\n",
      "    (method):  bert-attack\n",
      "    (masked_lm_name):  BertForMaskedLM\n",
      "    (max_length):  512\n",
      "    (max_candidates):  48\n",
      "    (min_confidence):  0.0005\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): MaxWordsPerturbed(\n",
      "        (max_percent):  0.4\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): WordEmbeddingDistance(\n",
      "        (embedding):  WordEmbedding\n",
      "        (min_cos_sim):  0.8\n",
      "        (cased):  False\n",
      "        (include_unknown_words):  True\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (2): RepeatModification\n",
      "    (3): StopwordModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 1 / 0 / 1:  10%|█         | 1/10 [00:00<00:05,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 1 ---------------------------------------------\n",
      "[[1 (100%)]] --> [[[FAILED]]]\n",
      "\n",
      "it 's a charming and often affecting journey . \n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- 4. 生成对抗样本 ---\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 注意：这一步会非常慢！\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m adversarial_df = \u001b[43mattacker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattack_recipe_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mATTACK_RECIPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTextAttack成功生成了 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(adversarial_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_SAMPLES_TO_TEST\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m 个对抗样本。\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# --- 4.5. 对齐攻击数据 ---\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks/../src/attacks/attacks.py:110\u001b[39m, in \u001b[36mAttackerWrapper.attack\u001b[39m\u001b[34m(self, dataset_for_attack, attack_recipe_name)\u001b[39m\n\u001b[32m    107\u001b[39m attacker = Attacker(recipe, attack_dataset)\n\u001b[32m    109\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m正在使用 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattack_recipe_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m 生成对抗样本...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m results_iterable = \u001b[43mattacker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattack_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m attacked_data = []\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m tqdm(results_iterable, total=\u001b[38;5;28mlen\u001b[39m(dataset_for_attack)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/textattack/attacker.py:441\u001b[39m, in \u001b[36mAttacker.attack_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    439\u001b[39m     \u001b[38;5;28mself\u001b[39m._attack_parallel()\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.attack_args.silent:\n\u001b[32m    444\u001b[39m     logger.setLevel(logging.INFO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/textattack/attacker.py:168\u001b[39m, in \u001b[36mAttacker._attack\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    166\u001b[39m     example.attack_attrs[\u001b[33m\"\u001b[39m\u001b[33mlabel_names\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.dataset.label_names\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattack\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/textattack/attack.py:450\u001b[39m, in \u001b[36mAttack.attack\u001b[39m\u001b[34m(self, example, ground_truth_output)\u001b[39m\n\u001b[32m    448\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SkippedAttackResult(goal_function_result)\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m450\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgoal_function_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/textattack/attack.py:398\u001b[39m, in \u001b[36mAttack._attack\u001b[39m\u001b[34m(self, initial_result)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_attack\u001b[39m(\u001b[38;5;28mself\u001b[39m, initial_result):\n\u001b[32m    388\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Calls the ``SearchMethod`` to perturb the ``AttackedText`` stored in\u001b[39;00m\n\u001b[32m    389\u001b[39m \u001b[33;03m    ``initial_result``.\u001b[39;00m\n\u001b[32m    390\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m            or ``MaximizedAttackResult``.\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     final_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msearch_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m     \u001b[38;5;28mself\u001b[39m.clear_cache()\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m final_result.goal_status == GoalFunctionResultStatus.SUCCEEDED:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/textattack/search_methods/search_method.py:36\u001b[39m, in \u001b[36mSearchMethod.__call__\u001b[39m\u001b[34m(self, initial_result)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfilter_transformations\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m     33\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSearch Method must have access to filter_transformations method\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     34\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mperform_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# ensure that the number of queries for this GoalFunctionResult is up-to-date\u001b[39;00m\n\u001b[32m     38\u001b[39m result.num_queries = \u001b[38;5;28mself\u001b[39m.goal_function.num_queries\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/textattack/search_methods/greedy_word_swap_wir.py:141\u001b[39m, in \u001b[36mGreedyWordSwapWIR.perform_search\u001b[39m\u001b[34m(self, initial_result)\u001b[39m\n\u001b[32m    139\u001b[39m results = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m i < \u001b[38;5;28mlen\u001b[39m(index_order) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m search_over:\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     transformed_text_candidates = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_transformations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcur_result\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattacked_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_result\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattacked_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindices_to_modify\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex_order\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m     i += \u001b[32m1\u001b[39m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(transformed_text_candidates) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/textattack/attack.py:305\u001b[39m, in \u001b[36mAttack.get_transformations\u001b[39m\u001b[34m(self, current_text, original_text, **kwargs)\u001b[39m\n\u001b[32m    303\u001b[39m     transformed_texts = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.transformation_cache[cache_key])\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m     transformed_texts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_transformations_uncached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    308\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m utils.hashable(cache_key):\n\u001b[32m    309\u001b[39m         \u001b[38;5;28mself\u001b[39m.transformation_cache[cache_key] = \u001b[38;5;28mtuple\u001b[39m(transformed_texts)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/textattack/attack.py:273\u001b[39m, in \u001b[36mAttack._get_transformations_uncached\u001b[39m\u001b[34m(self, current_text, original_text, **kwargs)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_transformations_uncached\u001b[39m(\u001b[38;5;28mself\u001b[39m, current_text, original_text=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    264\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Applies ``self.transformation`` to ``text``, then filters the list\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[33;03m    of possible transformations through the applicable constraints.\u001b[39;00m\n\u001b[32m    266\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    271\u001b[39m \u001b[33;03m        A filtered list of transformations where each transformation matches the constraints\u001b[39;00m\n\u001b[32m    272\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     transformed_texts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpre_transformation_constraints\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpre_transformation_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m transformed_texts\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/textattack/transformations/transformation.py:57\u001b[39m, in \u001b[36mTransformation.__call__\u001b[39m\u001b[34m(self, current_text, pre_transformation_constraints, indices_to_modify, shifted_idxs, return_indices)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_indices:\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m indices_to_modify\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m transformed_texts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_transformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_to_modify\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m transformed_texts:\n\u001b[32m     59\u001b[39m     text.attack_attrs[\u001b[33m\"\u001b[39m\u001b[33mlast_transformation\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/textattack/transformations/word_swaps/word_swap_masked_lm.py:266\u001b[39m, in \u001b[36mWordSwapMaskedLM._get_transformations\u001b[39m\u001b[34m(self, current_text, indices_to_modify)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices_to_modify:\n\u001b[32m    265\u001b[39m     word_at_index = current_text.words[i]\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     replacement_words = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bert_attack_replacement_words\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mid_preds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mid_preds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmasked_lm_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmasked_lm_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m replacement_words:\n\u001b[32m    274\u001b[39m         r = r.strip(\u001b[33m\"\u001b[39m\u001b[33mĠ\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/textattack/transformations/word_swaps/word_swap_masked_lm.py:237\u001b[39m, in \u001b[36mWordSwapMaskedLM._bert_attack_replacement_words\u001b[39m\u001b[34m(self, current_text, index, id_preds, masked_lm_logits)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(bpe_tokens)):\n\u001b[32m    235\u001b[39m     word_tensor[i] = bpe_tokens[i]\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m logits = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_lm_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_ids_pos_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m loss = cross_entropy_loss(logits, word_tensor)\n\u001b[32m    239\u001b[39m perplexity = torch.exp(torch.mean(loss, dim=\u001b[32m0\u001b[39m)).item()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- 4. 生成对抗样本 ---\n",
    "# 注意：这一步会非常慢！\n",
    "adversarial_df = attacker.attack(dataset, attack_recipe_name=ATTACK_RECIPE)\n",
    "print(f\"\\nTextAttack成功生成了 {len(adversarial_df)} / {NUM_SAMPLES_TO_TEST} 个对抗样本。\")\n",
    "\n",
    "# --- 4.5. 对齐攻击数据 ---\n",
    "attack_map = pd.Series(adversarial_df.perturbed_text.values, index=adversarial_df.original_text).to_dict()\n",
    "full_perturbed_texts = [attack_map.get(sent, sent) for sent in dataset_df['sentence']]\n",
    "print(f\"已生成与原始数据对齐的完整对抗样本列表，长度为: {len(full_perturbed_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41470b3c-3bbb-4a3a-90a9-cf1d8cfdc786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. 评估所有防御策略 ---\n",
    "results = []\n",
    "baseline_clean_preds = []\n",
    "\n",
    "for defense_name, defense_method in defenses.items():\n",
    "    print(f\"\\n{'='*20} 正在评估防御策略: {defense_name} {'='*20}\")\n",
    "\n",
    "    # a. 评估Clean Accuracy (在原始数据上)\n",
    "    clean_preds = [defense_method(text) for text in tqdm(dataset_df['sentence'], desc=\"Clean Eval\")]\n",
    "    clean_accuracy = calculate_accuracy(dataset_df['label_text'], clean_preds)\n",
    "\n",
    "    # b. 评估Accuracy under Attack (在对抗样本上)\n",
    "    attack_preds = [defense_method(text) for text in tqdm(full_perturbed_texts, desc=\"Attack Eval\")]\n",
    "    attack_accuracy = calculate_accuracy(dataset_df['label_text'], attack_preds)\n",
    "\n",
    "    # c. 计算ASR\n",
    "    if not baseline_clean_preds:\n",
    "        # 确保基线预测只计算一次\n",
    "        baseline_clean_preds = clean_preds if defense_name == \"No Defense (Baseline)\" else \\\n",
    "                               [defenses[\"No Defense (Baseline)\"](text) for text in tqdm(dataset_df['sentence'], desc=\"Baseline Eval\")]\n",
    "\n",
    "    attack_success_rate = calculate_asr(baseline_clean_preds, attack_preds, dataset_df['label_text'].tolist())\n",
    "\n",
    "    results.append({\n",
    "        \"防御方法 (Defense)\": defense_name,\n",
    "        \"原始准确率 (Clean Acc)\": clean_accuracy,\n",
    "        \"攻击后准确率 (Attack Acc)\": attack_accuracy,\n",
    "        \"攻击成功率 (ASR)\": attack_success_rate\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8702141a-fcdd-4c64-9fd0-886ad372f991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. 展示结果 ---\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n实验二：经验鲁棒性评估 - 结果汇总\")\n",
    "print(\"=\" * 70)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3e3781-0a0a-43c2-84c1-dc879bd21056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. 保存结果 ---\n",
    "if not os.path.exists('../results'):\n",
    "    os.makedirs('../results')\n",
    "\n",
    "# --- 自动编号并保存结果 ---\n",
    "if not os.path.exists('../results'):\n",
    "    os.makedirs('../results')\n",
    "\n",
    "base_path = f'../results/experiment_2_robustness_{ATTACK_RECIPE}'\n",
    "extension = '.csv'\n",
    "save_path = f\"{base_path}{extension}\"\n",
    "\n",
    "counter = 1\n",
    "while os.path.exists(save_path):\n",
    "    save_path = f\"{base_path}_{counter}{extension}\"\n",
    "    counter += 1\n",
    "\n",
    "results_df.to_csv(save_path, index=False)\n",
    "print(f\"\\n实验结果已成功保存到: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25394818-128b-4213-9f64-a7228bcb337a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AHP-Env)",
   "language": "python",
   "name": "ahp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
