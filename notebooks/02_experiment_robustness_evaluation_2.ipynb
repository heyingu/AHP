{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cae37e2c-c9a6-4c8a-9f0a-c261b1cb88c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 23 19:01:17 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.04             Driver Version: 570.124.04     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA vGPU-48GB               On  |   00000000:5B:00.0 Off |                  Off |\n",
      "|  0%   34C    P8             20W /  425W |       1MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fd2c56c-d1d4-43c2-aed1-2f27165ce07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 补丁1生效：已成功注入“全功能”伪模块。\n",
      ">>> 补丁2生效：已成功替换 nltk.download 函数。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 19:10:24.442419: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-23 19:10:24.496396: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-23 19:10:25.397942: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/root/ahp_env/lib/python3.12/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 恭喜！环境设置最终完成，所有模块导入成功！---\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# |      !!! 终极解决方案：“直捣黄龙”之最终极补丁 !!!             |\n",
    "# ====================================================================\n",
    "import sys\n",
    "import os\n",
    "from unittest.mock import patch\n",
    "import types\n",
    "from importlib.machinery import ModuleSpec\n",
    "\n",
    "# --- 补丁1：注入“全功能”伪模块 ---\n",
    "def create_full_fake_module(name, attributes_to_add):\n",
    "    spec = ModuleSpec(name, None)\n",
    "    module = types.ModuleType(name)\n",
    "    module.__spec__ = spec\n",
    "    for attr in attributes_to_add:\n",
    "        setattr(module, attr, lambda *args, **kwargs: None)\n",
    "    return module\n",
    "\n",
    "sys.modules['num2words'] = create_full_fake_module('num2words', ['num2words'])\n",
    "sys.modules['word2number'] = create_full_fake_module('word2number', ['w2n'])\n",
    "print(\">>> 补丁1生效：已成功注入“全功能”伪模块。\")\n",
    "\n",
    "# --- 补丁2：“直捣黄龙”，直接替换nltk.download函数 ---\n",
    "def dummy_nltk_download(*args, **kwargs):\n",
    "    print(\">>> 补丁2生效：已成功拦截并跳过 nltk.download() 调用！<<<\")\n",
    "    return True # 返回成功状态\n",
    "\n",
    "patcher = patch('nltk.download', dummy_nltk_download)\n",
    "patcher.start()\n",
    "print(\">>> 补丁2生效：已成功替换 nltk.download 函数。\")\n",
    "\n",
    "\n",
    "# --- 补丁3：手动为NLTK“指路” ---\n",
    "nltk_data_dir = os.path.expanduser('~/nltk_data')\n",
    "import nltk\n",
    "if nltk_data_dir not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_dir)\n",
    "    print(f\"成功将 '{nltk_data_dir}' 添加到NLTK的搜索路径。\")\n",
    "\n",
    "# --- 补丁4：设置其他环境变量 ---\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "sys.path.append('..')\n",
    "# ====================================================================\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- 正常导入所有模块 ---\n",
    "# 导入在 defenses.py 中定义的类\n",
    "from src.defenses import BasePredictor, NoDefense, AhpDefense, SelfDenoiseDefense\n",
    "from src.utils.data_loader import load_sst2_dataset\n",
    "from src.models.model_loader import load_main_llm\n",
    "from src.attacks import AttackerWrapper\n",
    "from src.utils.metrics import calculate_accuracy, calculate_asr\n",
    "\n",
    "print(\"\\n--- 恭喜！环境设置最终完成，所有模块导入成功！---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e2df9be-4938-4a90-82d7-a620824d4332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 实验设置 ---\n",
    "TASK = 'sst2'\n",
    "DATASET_NAME = 'SST-2'\n",
    "NUM_SAMPLES_TO_TEST = 50 \n",
    "ATTACK_RECIPE = 'textbugger' # 可以切换为 'textbugger'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebbd5efc-dfd2-4ba8-b97c-2ad6ae929957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载一个更小的、用于公平对比的基础模型: distilbert-base-uncased-finetuned-sst-2-english...\n",
      "正在加载SST-2数据集 (validation split)...\n",
      "SST-2数据集加载成功。\n"
     ]
    }
   ],
   "source": [
    "# --- 2. 加载模型和数据 ---\n",
    "# 请确保这里的路径是您正在使用的 alpaca-7b 模型的正确路径\n",
    "local_model_path = \"/root/autodl-tmp/circulus_alpaca-7b\"\n",
    "# 使用4-bit量化加载模型，这是节省显存的第一道防线\n",
    "main_model, main_tokenizer = load_main_llm(model_name=local_model_path, use_4bit=True)\n",
    "# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# # 我们不再使用 load_main_llm，而是直接加载一个标准的分类模型\n",
    "# MODEL_NAME = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# print(f\"正在加载一个更小的、用于公平对比的基础模型: {MODEL_NAME}...\")\n",
    "\n",
    "# 加载模型和分词器\n",
    "main_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "main_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "main_model.to(\"cuda\") # 将模型移动到GPU\n",
    "\n",
    "dataset = load_sst2_dataset(split='validation').select(range(NUM_SAMPLES_TO_TEST))\n",
    "dataset_df = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db87408c-87c9-4ba6-a314-f1ef44daa8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基础模型加载并适配完成！\n",
      "正在加载NLI模型: roberta-large-mnli...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI模型加载成功。\n"
     ]
    }
   ],
   "source": [
    "# --- 3. 初始化防御策略和攻击器 ---\n",
    "# class SimplePredictor:\n",
    "#     def __init__(self, model, tokenizer):\n",
    "#         self.model = model\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.device = model.device\n",
    "\n",
    "#     def predict(self, sentence):\n",
    "#         inputs = self.tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
    "#         with torch.no_grad():\n",
    "#             logits = self.model(**inputs).logits\n",
    "        \n",
    "#         predicted_class_id = torch.argmax(logits, dim=1).item()\n",
    "#         # 对于 SST-2, 0 是 'negative', 1 是 'positive'\n",
    "#         return \"positive\" if predicted_class_id == 1 else \"negative\"\n",
    "# 创建基础预测器，所有防御和攻击都将基于它\n",
    "# 这里的 BasePredictor 来自于您提供的 defenses.py 文件\n",
    "base_predictor = BasePredictor(main_model, main_tokenizer, task=TASK)\n",
    "# base_predictor = SimplePredictor(main_model, main_tokenizer)\n",
    "\n",
    "print(\"基础模型加载并适配完成！\")\n",
    "\n",
    "# ===================== 关键修正：降低AHP防御的显存消耗 =====================\n",
    "# 将 m_val 从 10 大幅减少到 5。\n",
    "# 这是解决 CUDA_ERROR_INVALID_HANDLE (显存不足) 错误的核心步骤。\n",
    "# 因为 AHP 防御需要一次性生成 m_val 个候选句子，这个值过大会耗尽显存。\n",
    "AHP_M_VAL = 5 \n",
    "# =======================================================================\n",
    "\n",
    "# 定义要对比的防御策略\n",
    "defenses = {\n",
    "    \"No Defense (Baseline)\": NoDefense(base_predictor),\n",
    "    \"AHP-NLI Defense\": AhpDefense(base_predictor, k_val=3, m_val=AHP_M_VAL),\n",
    "    \"Self-Denoise Defense\": SelfDenoiseDefense(base_predictor, num_samples=10)\n",
    "}\n",
    "\n",
    "# 初始化攻击器\n",
    "# 这里的 AttackerWrapper 来自于您提供的 attacks/attacks.py 文件\n",
    "attacker = AttackerWrapper(base_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bade34d8-7710-46fb-978a-b66375909a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedyWordSwapWIR(\n",
      "    (wir_method):  delete\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  CompositeTransformation(\n",
      "    (0): WordSwapRandomCharacterInsertion(\n",
      "        (random_one):  True\n",
      "      )\n",
      "    (1): WordSwapRandomCharacterDeletion(\n",
      "        (random_one):  True\n",
      "      )\n",
      "    (2): WordSwapNeighboringCharacterSwap(\n",
      "        (random_one):  True\n",
      "      )\n",
      "    (3): WordSwapHomoglyphSwap\n",
      "    (4): WordSwapEmbedding(\n",
      "        (max_candidates):  5\n",
      "        (embedding):  WordEmbedding\n",
      "      )\n",
      "    )\n",
      "  (constraints): \n",
      "    (0): UniversalSentenceEncoder(\n",
      "        (metric):  angular\n",
      "        (threshold):  0.8\n",
      "        (window_size):  inf\n",
      "        (skip_text_shorter_than_window):  False\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): RepeatModification\n",
      "    (2): StopwordModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1761217883.415565    4091 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 44808 MB memory:  -> device: 0, name: NVIDIA vGPU-48GB, pci bus id: 0000:5b:00.0, compute capability: 8.9\n",
      "[Succeeded / Failed / Skipped / Total] 32 / 17 / 1 / 50: 100%|██████████| 50/50 [00:12<00:00,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 32     |\n",
      "| Number of failed attacks:     | 17     |\n",
      "| Number of skipped attacks:    | 1      |\n",
      "| Original accuracy:            | 98.0%  |\n",
      "| Accuracy under attack:        | 34.0%  |\n",
      "| Attack success rate:          | 65.31% |\n",
      "| Average perturbed word %:     | 12.94% |\n",
      "| Average num. words per input: | 16.32  |\n",
      "| Avg num queries:              | 38.2   |\n",
      "+-------------------------------+--------+\n",
      "正在使用 textbugger (黑盒模式) 生成对抗样本...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003d518706f44198bb0b347e5172350c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TextAttack成功生成了 50 / 50 个对抗样本。\n",
      "已生成与原始数据对齐的完整对抗样本列表，长度为: 50\n"
     ]
    }
   ],
   "source": [
    "# --- 4. 生成对抗样本 ---\n",
    "# 注意：这一步会非常慢！\n",
    "adversarial_df = attacker.attack(dataset, attack_recipe_name=ATTACK_RECIPE)\n",
    "print(f\"\\nTextAttack成功生成了 {len(adversarial_df)} / {NUM_SAMPLES_TO_TEST} 个对抗样本。\")\n",
    "\n",
    "# --- 4.5. 对齐攻击数据 ---\n",
    "attack_map = pd.Series(adversarial_df.perturbed_text.values, index=adversarial_df.original_text).to_dict()\n",
    "full_perturbed_texts = [attack_map.get(sent, sent) for sent in dataset_df['sentence']]\n",
    "print(f\"已生成与原始数据对齐的完整对抗样本列表，长度为: {len(full_perturbed_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41470b3c-3bbb-4a3a-90a9-cf1d8cfdc786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== 正在评估防御策略: No Defense (Baseline) ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d31b1bfe2f35437c8dc05d4883799cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean Eval:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e4013c7209469a8ff62d2bd9bed871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Attack Eval:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== 正在评估防御策略: AHP-NLI Defense ====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff38a57cab0c4cf396ba07a669f57b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean Eval:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (12).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m 正在评估防御策略: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdefense_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# a. 评估Clean Accuracy (在原始数据上)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m clean_preds = [\u001b[43mdefense_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tqdm(dataset_df[\u001b[33m'\u001b[39m\u001b[33msentence\u001b[39m\u001b[33m'\u001b[39m], desc=\u001b[33m\"\u001b[39m\u001b[33mClean Eval\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m     10\u001b[39m clean_accuracy = calculate_accuracy(dataset_df[\u001b[33m'\u001b[39m\u001b[33mlabel_text\u001b[39m\u001b[33m'\u001b[39m], clean_preds)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# b. 评估Accuracy under Attack (在对抗样本上)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks/../src/defenses.py:89\u001b[39m, in \u001b[36mAhpDefense.__call__\u001b[39m\u001b[34m(self, sentence)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence):\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     masked_text = \u001b[43madversarial_masking\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m     candidates = \u001b[38;5;28mself\u001b[39m.candidate_generator.generate(masked_text)\n\u001b[32m     91\u001b[39m     pruned_candidates = \u001b[38;5;28mself\u001b[39m.pruner.prune(sentence, candidates)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/notebooks/../src/components/masking.py:53\u001b[39m, in \u001b[36madversarial_masking\u001b[39m\u001b[34m(text, model, tokenizer, mask_ratio)\u001b[39m\n\u001b[32m     49\u001b[39m input_embeds.retain_grad()\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# ===================== 修正结束 =====================\u001b[39;00m\n\u001b[32m     51\u001b[39m \n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# 2. 前向传播计算损失 (使用语言模型自身的损失)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m loss = outputs.loss\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# 3. 反向传播计算梯度\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/transformers/models/distilbert/modeling_distilbert.py:939\u001b[39m, in \u001b[36mDistilBertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    937\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.problem_type == \u001b[33m\"\u001b[39m\u001b[33msingle_label_classification\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    938\u001b[39m     loss_fct = CrossEntropyLoss()\n\u001b[32m--> \u001b[39m\u001b[32m939\u001b[39m     loss = \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.problem_type == \u001b[33m\"\u001b[39m\u001b[33mmulti_label_classification\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    941\u001b[39m     loss_fct = BCEWithLogitsLoss()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:1310\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1309\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m-> \u001b[39m\u001b[32m1310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1311\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/torch/nn/functional.py:3462\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3461\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3462\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3466\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3469\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: Expected input batch_size (1) to match target batch_size (12)."
     ]
    }
   ],
   "source": [
    "# --- 5. 评估所有防御策略 ---\n",
    "results = []\n",
    "baseline_clean_preds = []\n",
    "\n",
    "for defense_name, defense_method in defenses.items():\n",
    "    print(f\"\\n{'='*20} 正在评估防御策略: {defense_name} {'='*20}\")\n",
    "\n",
    "    # a. 评估Clean Accuracy (在原始数据上)\n",
    "    clean_preds = [defense_method(text) for text in tqdm(dataset_df['sentence'], desc=\"Clean Eval\")]\n",
    "    clean_accuracy = calculate_accuracy(dataset_df['label_text'], clean_preds)\n",
    "\n",
    "    # b. 评估Accuracy under Attack (在对抗样本上)\n",
    "    attack_preds = [defense_method(text) for text in tqdm(full_perturbed_texts, desc=\"Attack Eval\")]\n",
    "    attack_accuracy = calculate_accuracy(dataset_df['label_text'], attack_preds)\n",
    "\n",
    "    # c. 计算ASR\n",
    "    if not baseline_clean_preds:\n",
    "        # 确保基线预测只计算一次\n",
    "        baseline_clean_preds = clean_preds if defense_name == \"No Defense (Baseline)\" else \\\n",
    "                               [defenses[\"No Defense (Baseline)\"](text) for text in tqdm(dataset_df['sentence'], desc=\"Baseline Eval\")]\n",
    "\n",
    "    attack_success_rate = calculate_asr(baseline_clean_preds, attack_preds, dataset_df['label_text'].tolist())\n",
    "\n",
    "    results.append({\n",
    "        \"防御方法 (Defense)\": defense_name,\n",
    "        \"原始准确率 (Clean Acc)\": clean_accuracy,\n",
    "        \"攻击后准确率 (Attack Acc)\": attack_accuracy,\n",
    "        \"攻击成功率 (ASR)\": attack_success_rate\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8702141a-fcdd-4c64-9fd0-886ad372f991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "实验二：经验鲁棒性评估 - 结果汇总\n",
      "======================================================================\n",
      "       防御方法 (Defense)  原始准确率 (Clean Acc)  攻击后准确率 (Attack Acc)  攻击成功率 (ASR)\n",
      "No Defense (Baseline)               0.78                 0.78     0.000000\n",
      "      AHP-NLI Defense               0.84                 0.88     0.076923\n",
      " Self-Denoise Defense               0.80                 0.84     0.025641\n"
     ]
    }
   ],
   "source": [
    "# --- 6. 展示结果 ---\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n实验二：经验鲁棒性评估 - 结果汇总\")\n",
    "print(\"=\" * 70)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b3e3781-0a0a-43c2-84c1-dc879bd21056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "实验结果已成功保存到: ../results/experiment_2_robustness_textbugger_3.csv\n"
     ]
    }
   ],
   "source": [
    "# --- 7. 保存结果 ---\n",
    "if not os.path.exists('../results'):\n",
    "    os.makedirs('../results')\n",
    "\n",
    "# --- 自动编号并保存结果 ---\n",
    "if not os.path.exists('../results'):\n",
    "    os.makedirs('../results')\n",
    "\n",
    "base_path = f'../results/experiment_2_robustness_{ATTACK_RECIPE}'\n",
    "extension = '.csv'\n",
    "save_path = f\"{base_path}{extension}\"\n",
    "\n",
    "counter = 1\n",
    "while os.path.exists(save_path):\n",
    "    save_path = f\"{base_path}_{counter}{extension}\"\n",
    "    counter += 1\n",
    "\n",
    "results_df.to_csv(save_path, index=False)\n",
    "print(f\"\\n实验结果已成功保存到: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25394818-128b-4213-9f64-a7228bcb337a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AHP-Env)",
   "language": "python",
   "name": "ahp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
