{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9e44ea8-eeb0-4a10-865b-f92e04e82abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-07 14:00:43.675855: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-07 14:00:44.237513: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-07 14:00:45.679783: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/root/ahp_env/lib/python3.12/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # 实验 3: 运行多种 TextAttack 攻击方法\n",
    "#\n",
    "# **目标:** 方便地测试不同的 TextAttack 攻击配方 (recipes) 针对 Alpaca-7B 模型的效果，可以应用不同的防御策略（无防御、AHP、SelfDenoise）。\n",
    "#\n",
    "# **方法:**\n",
    "# 1. 设置基础环境和代理（如果需要）。\n",
    "# 2. 定义一个函数 `run_single_experiment`，该函数接收攻击方法、防御方法等核心参数，并执行一次完整的攻击流程。\n",
    "# 3. 在 Notebook 中定义要测试的攻击方法列表和防御方法列表。\n",
    "# 4. 循环调用 `run_single_experiment` 函数来执行所有组合，或手动修改参数运行单个实验。\n",
    "# 5. 加载并显示最终的结果 CSV 文件。\n",
    "import os\n",
    "# os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "# os.environ[\"TEXTATTACK_OFFLINE\"] = \"1\"\n",
    "\n",
    "# print(\"已设置 TRANSFORMERS_OFFLINE=1 (强制使用本地缓存)\")\n",
    "# print(\"已设置 TEXTATTACK_OFFLINE=1 (强制使用本地缓存)\")\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "os.environ[\"DATASETS_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "\n",
    "# %% 导入基础库\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import logging\n",
    "from IPython.display import display # 用于在 Notebook 中美观地显示 DataFrame\n",
    "import gc\n",
    "import random\n",
    "# %% [markdown]\n",
    "# ## 1. 环境设置与代理配置 (如果需要)\n",
    "\n",
    "# %%\n",
    "# --- 应用 AutoDL 网络加速配置 (如果在此环境) ---\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "# logging.info(\"正在应用 AutoDL 网络加速配置...\")\n",
    "# try:\n",
    "#     result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"',\n",
    "#                             shell=True, capture_output=True, text=True, check=True)\n",
    "#     output = result.stdout\n",
    "#     proxy_found = False\n",
    "#     for line in output.splitlines():\n",
    "#         if '=' in line:\n",
    "#             var, value = line.split('=', 1)\n",
    "#             value = value.strip('\\'\"')\n",
    "#             os.environ[var] = value\n",
    "#             logging.info(f\"已设置环境变量: {var}={value}\")\n",
    "#             proxy_found = True\n",
    "#     if not proxy_found:\n",
    "#         logging.warning(\"未能从 /etc/network_turbo 获取到代理环境变量。\")\n",
    "#     else:\n",
    "#         print(\"AutoDL 网络加速配置应用成功。\")\n",
    "#         logging.info(\"AutoDL 网络加速配置应用成功。\")\n",
    "# except FileNotFoundError:\n",
    "#     logging.warning(\"/etc/network_turbo 文件不存在，跳过代理设置。可能不在 AutoDL 环境中。\")\n",
    "# except subprocess.CalledProcessError as e:\n",
    "#     logging.error(f\"执行 AutoDL 网络配置命令失败: {e}\\n{e.stderr}\")\n",
    "# except Exception as e:\n",
    "#     logging.error(f\"应用 AutoDL 网络配置时发生未知错误: {e}\")\n",
    "\n",
    "# --- 将 src 目录添加到 Python 路径 ---\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    logging.info(f\"已将 '{module_path}' 添加到 sys.path\")\n",
    "\n",
    "# --- 导入项目代码 ---\n",
    "# 确保在设置代理和路径后再导入\n",
    "try:\n",
    "    import torch\n",
    "    import transformers\n",
    "    import datasets\n",
    "    from src.args_config import AHPSettings\n",
    "    from src.experiment_runner import ExperimentRunner\n",
    "    logging.info(\"项目模块导入成功。\")\n",
    "except ImportError as e:\n",
    "    logging.error(f\"导入项目模块失败: {e}\", exc_info=True)\n",
    "    # 如果导入失败，后续代码可能无法运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56039fe3-857c-4398-b1b5-0e2dfdd95f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 2. 定义实验运行函数\n",
    "#\n",
    "# 这个函数封装了运行一次攻击实验（或评估）的逻辑。\n",
    "\n",
    "# %%\n",
    "def run_single_experiment(\n",
    "    # --- 核心可变参数 ---\n",
    "    attack_method: str = 'textbugger', # 要使用的 TextAttack 配方\n",
    "    defense_method: str = 'none',      # 要使用的防御方法 ('none', 'ahp', 'selfdenoise')\n",
    "    dataset_name: str = 'agnews',        # 数据集 ('sst2', 'agnews')\n",
    "    num_examples: int = 50,           # 测试样本数量 (建议先用少量测试)\n",
    "\n",
    "    # --- 固定或较少变动的参数 (根据需要修改默认值) ---\n",
    "    mode: str = 'attack',              # 运行模式 ('attack' 或 'evaluate')\n",
    "    model_path: str = '/root/autodl-tmp/alpaca-native', # <--- !!! 请务必确认并修改为您的正确模型路径 !!!\n",
    "    dataset_path: str = '../dataset',     # 数据集根目录\n",
    "    results_file: str = '../results/experiment_results_multi_attack.csv', # 结果汇总文件\n",
    "    attack_log_path: str = '../results/multi_attack_logs', # 详细攻击日志目录\n",
    "    cache_dir: str = '/root/autodl-tmp/cache_path',  # 缓存目录\n",
    "    model_batch_size: int = 4,         # 模型推理批次大小\n",
    "    max_seq_length: int = 128,         # 最大序列长度\n",
    "    mask_token: str = '<unk>',        # 遮蔽标记\n",
    "    mask_rate: float = 0.15,           # 遮蔽率\n",
    "    attack_query_budget: int = 100,    # 攻击查询预算\n",
    "    \n",
    "    # --- AHP 特定参数 ---\n",
    "    ahp_num_candidates: int = 10,\n",
    "    ahp_pruning_method: str = 'none',\n",
    "    ahp_pruning_threshold: float = 0.7,\n",
    "    ahp_aggregation_strategy: str = 'majority_vote',\n",
    "    ahp_masking_strategy = 'adversarial',#random\n",
    "    \n",
    "    # --- SelfDenoise 特定参数 ---\n",
    "    selfdenoise_ensemble_size: int = 10, # 集成大小 (适当减小以加速测试)\n",
    "    selfdenoise_denoiser: str = 'roberta', # 去噪器 ('alpaca', 'roberta')\n",
    "\n",
    "    # --- 环境参数 ---\n",
    "    seed: int = 42,\n",
    "    device: str = None, # 自动检测 cuda 或 cpu\n",
    "    log_level: str = 'INFO',\n",
    "\n",
    "    **kwargs # 允许传递其他未明确定义的参数 (如果 AHPSettings 支持)\n",
    "):\n",
    "    \"\"\"\n",
    "    运行单次实验（攻击或评估）。\n",
    "\n",
    "    Args:\n",
    "        (参数说明见函数定义)\n",
    "    \"\"\"\n",
    "    logging.info(f\"--- 开始实验: 数据集={dataset_name}, 防御={defense_method}, 攻击={attack_method if mode=='attack' else 'None'} ---\")\n",
    "\n",
    "    # --- 构建参数列表 ---\n",
    "    # 将函数参数转换为 run_experiment.py 所需的命令行参数格式\n",
    "    args_list = [\n",
    "        '--mode', mode,\n",
    "        '--dataset_name', dataset_name,\n",
    "        '--num_examples', str(num_examples),\n",
    "        '--model_path', model_path,\n",
    "        '--dataset_path', dataset_path,\n",
    "        '--results_file', results_file,\n",
    "        '--attack_log_path', attack_log_path,\n",
    "        '--cache_dir', cache_dir,\n",
    "        '--model_batch_size', str(model_batch_size),\n",
    "        '--max_seq_length', str(max_seq_length),\n",
    "        '--mask_token', mask_token,\n",
    "        '--mask_rate', str(mask_rate),\n",
    "        '--defense_method', defense_method,\n",
    "        '--seed', str(seed),\n",
    "        '--log_level', log_level,\n",
    "    ]\n",
    "    # 添加设备参数 (如果指定了)\n",
    "    if device:\n",
    "        args_list.extend(['--device', device])\n",
    "\n",
    "    # 根据模式添加攻击相关参数\n",
    "    if mode == 'attack':\n",
    "        args_list.extend([\n",
    "            '--attack_method', attack_method,\n",
    "            '--attack_query_budget', str(attack_query_budget),\n",
    "        ])\n",
    "\n",
    "    # 根据防御方法添加特定参数\n",
    "    if defense_method == 'ahp':\n",
    "        args_list.extend([\n",
    "            '--ahp_num_candidates', str(ahp_num_candidates),\n",
    "            '--ahp_pruning_method', ahp_pruning_method,\n",
    "            '--ahp_pruning_threshold', str(ahp_pruning_threshold),\n",
    "            '--ahp_aggregation_strategy', ahp_aggregation_strategy,\n",
    "        ])\n",
    "    elif defense_method == 'selfdenoise':\n",
    "        args_list.extend([\n",
    "            '--selfdenoise_ensemble_size', str(selfdenoise_ensemble_size),\n",
    "            '--selfdenoise_denoiser', selfdenoise_denoiser,\n",
    "        ])\n",
    "\n",
    "    # 添加 **kwargs 中的额外参数 (如果需要)\n",
    "    # for k, v in kwargs.items():\n",
    "    #    args_list.extend([f'--{k}', str(v)]) # 需要确保 AHPSettings 支持这些参数\n",
    "\n",
    "    logging.debug(f\"构建的参数列表: {' '.join(args_list)}\")\n",
    "\n",
    "    # --- 解析参数并运行实验 ---\n",
    "    try:\n",
    "        # 1. 解析参数\n",
    "        # AHPSettings().parse_args(args_list) 会设置日志、设备、种子等\n",
    "        args = AHPSettings().parse_args(args_list)\n",
    "        logging.info(\"参数解析完成，开始初始化 ExperimentRunner...\")\n",
    "\n",
    "        # 2. 初始化 Runner\n",
    "        # ExperimentRunner 内部会初始化 AlpacaModel\n",
    "        runner = ExperimentRunner(args)\n",
    "        logging.info(\"ExperimentRunner 初始化完成，开始运行...\")\n",
    "\n",
    "        # 3. 运行实验 (attack 或 evaluate)\n",
    "        runner.run()\n",
    "        logging.info(f\"--- 实验完成: 数据集={dataset_name}, 防御={defense_method}, 攻击={attack_method if mode=='attack' else 'None'} ---\")\n",
    "        return True # 表示成功\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"--- 实验失败: 数据集={dataset_name}, 防御={defense_method}, 攻击={attack_method if mode=='attack' else 'None'} ---\")\n",
    "        logging.error(f\"错误信息: {e}\", exc_info=True) # exc_info=True 打印详细错误堆栈\n",
    "        return False # 表示失败"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af5e04ae-20c3-4778-b3d0-776fe1553d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 3. 定义要测试的攻击和防御组合\n",
    "\n",
    "# %%\n",
    "# --- 要测试的攻击方法列表 ---\n",
    "# (基于您之前的讨论和代码中的可用性)\n",
    "# 确保您已经在 src/experiment_runner.py 和 src/args_config.py 中添加了对 'bertattack' 的支持\n",
    "attack_methods_to_test = [\n",
    "    # 'textbugger',\n",
    "    # 'textfooler',\n",
    "    # 'pwws',\n",
    "    'deepwordbug',\n",
    "    # ,\n",
    "    # 'bae', # BAE 速度较慢，可以取消注释以包含它\n",
    "    # 'bertattack', # 需要确保已集成\n",
    "]\n",
    "\n",
    "# --- 要测试的防御方法列表 ---\n",
    "defense_methods_to_test = [\n",
    "    'none',        # 无防御基线\n",
    "    'ahp',         # AHP 防御\n",
    "    'selfdenoise', # SelfDenoise 防御\n",
    "]\n",
    "\n",
    "# --- 要测试的数据集 ---\n",
    "datasets_to_test = [\n",
    "    'sst2',\n",
    "    # 'agnews', # 可以取消注释以测试 AG News\n",
    "]\n",
    "\n",
    "# --- 其他通用参数 ---\n",
    "# !!! 务必修改为正确的模型路径 !!!\n",
    "common_params = {\n",
    "    \"num_examples\": 100, # 设置一个合理的测试样本数量\n",
    "    \"model_path\": '/root/autodl-tmp/alpaca-native', # <--- !!! 再次确认路径 !!!\n",
    "    \"results_file\": '../results/experiment_results_multi_attack.csv', # 所有结果汇总到这里\n",
    "    \"attack_log_path\": '../results/multi_attack_logs', # 每个实验的详细日志子目录\n",
    "    \"mask_rate\": 0.15, # 通用遮蔽率\n",
    "    \"attack_query_budget\": 200, # 查询预算\n",
    "    \"ahp_pruning_method\": \"none\", # AHP 默认剪枝方法 semantic\n",
    "    \"ahp_aggregation_strategy\": \"majority_vote\", # AHP 默认聚合策略\n",
    "    \"selfdenoise_denoiser\": \"alpaca\", # SelfDenoise 默认去噪器\n",
    "    \"selfdenoise_ensemble_size\": 10, # SelfDenoise 默认集成大小\n",
    "    \"log_level\": \"INFO\", # 日志级别\n",
    "    # \"ahp_masking_strategy\": \"random\",#random adversarial\n",
    "}\n",
    "\n",
    "# --- 清理旧的结果文件 (可选) ---\n",
    "# 如果希望每次运行都生成全新的结果文件，可以在这里删除旧文件\n",
    "results_filepath = common_params[\"results_file\"]\n",
    "# if os.path.exists(results_filepath):\n",
    "#     logging.warning(f\"正在删除旧的结果文件: {results_filepath}\")\n",
    "#     os.remove(results_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18099b99-bfbf-49b0-8495-0e3ab840fec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %% [markdown]\n",
    "# # ## 4. 循环运行实验 (自动运行所有组合)\n",
    "# #\n",
    "# # **警告:** 运行所有组合可能需要很长时间！建议先使用少量样本 (`num_examples`) 和部分攻击/防御方法进行测试。\n",
    "\n",
    "# # %%\n",
    "# # --- 循环遍历所有组合 ---\n",
    "# for dataset in datasets_to_test:\n",
    "#     for defense in defense_methods_to_test:\n",
    "#         # 首先运行一次干净样本评估 (mode='evaluate', attack='None')\n",
    "#         logging.info(f\"\\n===== 开始评估: Dataset={dataset}, Defense={defense} =====\")\n",
    "#         run_single_experiment(\n",
    "#             mode='evaluate', # 设置为评估模式\n",
    "#             attack_method='none', # 攻击方法设为 none 或其他占位符\n",
    "#             defense_method=defense,\n",
    "#             dataset_name=dataset,\n",
    "#             **common_params # 传递通用参数\n",
    "#         )\n",
    "\n",
    "#         # 然后运行所有指定的攻击方法\n",
    "#         for attack in attack_methods_to_test:\n",
    "#             # 特殊情况：如果防御是 'none' 且攻击也是 'none' (在评估中已完成)，则跳过\n",
    "#             if defense == 'none' and attack == 'none':\n",
    "#                 continue\n",
    "\n",
    "#             # BAE 攻击通常非常慢，可以选择性跳过\n",
    "#             # if attack == 'bae':\n",
    "#             #     logging.warning(\"跳过 BAE 攻击，因为它可能非常耗时。\")\n",
    "#             #     continue\n",
    "\n",
    "#             logging.info(f\"\\n===== 开始攻击: Dataset={dataset}, Defense={defense}, Attack={attack} =====\")\n",
    "#             run_single_experiment(\n",
    "#                 mode='attack', # 设置为攻击模式\n",
    "#                 attack_method=attack,\n",
    "#                 defense_method=defense,\n",
    "#                 dataset_name=dataset,\n",
    "#                 **common_params # 传递通用参数\n",
    "#             )\n",
    "#             # 添加一些延迟或显存清理（如果需要）\n",
    "#             # import time\n",
    "#             # time.sleep(5)\n",
    "#             if torch.cuda.is_available():\n",
    "#                 torch.cuda.empty_cache()\n",
    "#                 gc.collect()\n",
    "\n",
    "# logging.info(\"\\n<<<<< 所有实验组合运行完毕 >>>>>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58593067-936d-4ca2-8890-3e4d04f6664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6362c163-f0e7-4cd6-b85d-34e2b132e951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612de3613c2b4eefb6b88a7fbea7110a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "无防御预测:   0%|                                                             | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 0 / 1 / 1:   1%|          | 1/100 [00:00<00:05, 16.88it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcfe8a81db5047278f47d084870a9e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "无防御预测:   0%|                                                             | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 0 / 2 / 2:   2%|▏         | 2/100 [00:00<00:05, 16.72it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29bcb3aa60446e39a291ec76bd1754e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "无防御预测:   0%|                                                             | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b6a0d8dfae402d8e5883a8119526ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "无防御预测:   0%|                                                             | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-07 16:27:45.722211: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleLoadData(&module, data)' failed with 'CUDA_ERROR_INVALID_PTX'\n",
      "\n",
      "2025-11-07 16:27:45.722234: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleGetFunction(&function, module, kernel_name)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n",
      "\n",
      "ERROR:root:--- 实验失败: 数据集=agnews, 防御=none, 攻击=bertattack ---\n",
      "2025-11-07 16:27:45.722241: W tensorflow/core/framework/op_kernel.cc:1842] INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n",
      "2025-11-07 16:27:45.722260: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 12243618315410178558\n",
      "2025-11-07 16:27:45.722263: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 8524623954017694408\n",
      "2025-11-07 16:27:45.722266: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 11215440284654715396\n",
      "2025-11-07 16:27:45.722270: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n",
      "\t [[{{node text_preprocessor/Add}}]]\n",
      "\t [[StatefulPartitionedCall/StatefulPartitionedCall/text_preprocessor/Where/_40]]\n",
      "2025-11-07 16:27:45.722274: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9923046048821379555\n",
      "2025-11-07 16:27:45.722276: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 14236084637732971703\n",
      "2025-11-07 16:27:45.722280: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10422882723537188535\n",
      "2025-11-07 16:27:45.722283: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 8599797533024919937\n",
      "2025-11-07 16:27:45.722287: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 2282187583128792119\n",
      "2025-11-07 16:27:45.722293: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 8268522972978406546\n",
      "2025-11-07 16:27:45.722295: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 12185689543538714054\n",
      "2025-11-07 16:27:45.722299: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 14759171003877605882\n",
      "2025-11-07 16:27:45.722319: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 4887108686307416732\n",
      "2025-11-07 16:27:45.722321: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 16951310382504947156\n",
      "ERROR:root:错误信息: Graph execution error:\n",
      "\n",
      "Detected at node text_preprocessor/Add defined at (most recent call last):\n",
      "<stack traces unavailable>\n",
      "Detected at node text_preprocessor/Add defined at (most recent call last):\n",
      "<stack traces unavailable>\n",
      "2 root error(s) found.\n",
      "  (0) INTERNAL:  'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n",
      "\t [[{{node text_preprocessor/Add}}]]\n",
      "\t [[StatefulPartitionedCall/StatefulPartitionedCall/text_preprocessor/Where/_40]]\n",
      "  (1) INTERNAL:  'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n",
      "\t [[{{node text_preprocessor/Add}}]]\n",
      "0 successful operations.\n",
      "0 derived errors ignored. [Op:__inference_restored_function_body_16816]\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_1550/773100135.py\", line 116, in run_single_experiment\n",
      "    runner.run()\n",
      "  File \"/root/src/experiment_runner.py\", line 129, in run\n",
      "    self.attack() # 执行对抗攻击流程\n",
      "    ^^^^^^^^^^^^^\n",
      "  File \"/root/src/experiment_runner.py\", line 254, in attack\n",
      "    attack_iterator = tqdm(attacker.attack_dataset(), total=len(self.dataset), desc=\"正在攻击\", ncols=100)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/ahp_env/lib/python3.12/site-packages/textattack/attacker.py\", line 441, in attack_dataset\n",
      "    self._attack()\n",
      "  File \"/root/ahp_env/lib/python3.12/site-packages/textattack/attacker.py\", line 170, in _attack\n",
      "    raise e\n",
      "  File \"/root/ahp_env/lib/python3.12/site-packages/textattack/attacker.py\", line 168, in _attack\n",
      "    result = self.attack.attack(example, ground_truth_output)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/ahp_env/lib/python3.12/site-packages/textattack/attack.py\", line 450, in attack\n",
      "    result = self._attack(goal_function_result)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/ahp_env/lib/python3.12/site-packages/textattack/attack.py\", line 398, in _attack\n",
      "    final_result = self.search_method(initial_result)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/ahp_env/lib/python3.12/site-packages/textattack/search_methods/search_method.py\", line 36, in __call__\n",
      "    result = self.perform_search(initial_result)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/ahp_env/lib/python3.12/site-packages/textattack/search_methods/greedy_word_swap_wir.py\", line 141, in perform_search\n",
      "    transformed_text_candidates = self.get_transformations(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/ahp_env/lib/python3.12/site-packages/textattack/attack.py\", line 315, in get_transformations\n",
      "    return self.filter_transformations(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/ahp_env/lib/python3.12/site-packages/textattack/attack.py\", line 380, in filter_transformations\n",
      "    filtered_texts += self._filter_transformations_uncached(\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/ahp_env/lib/python3.12/site-packages/textattack/attack.py\", line 340, in _filter_transformations_uncached\n",
      "    filtered_texts = C.call_many(filtered_texts, original_text)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/ahp_env/lib/python3.12/site-packages/textattack/constraints/constraint.py\", line 50, in call_many\n",
      "    filtered_texts = self._check_constraint_many(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/ahp_env/lib/python3.12/site-packages/textattack/constraints/semantics/sentence_encoders/sentence_encoder.py\", line 179, in _check_constraint_many\n",
      "    scores = self._score_list(reference_text, transformed_texts)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/ahp_env/lib/python3.12/site-packages/textattack/constraints/semantics/sentence_encoders/sentence_encoder.py\", line 152, in _score_list\n",
      "    embeddings = self.encode(starting_text_windows + transformed_text_windows)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/ahp_env/lib/python3.12/site-packages/textattack/constraints/semantics/sentence_encoders/universal_sentence_encoder/universal_sentence_encoder.py\", line 31, in encode\n",
      "    encoding = self.model(sentences)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/ahp_env/lib/python3.12/site-packages/tensorflow/python/saved_model/load.py\", line 817, in _call_attribute\n",
      "    return instance.__call__(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/ahp_env/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/root/ahp_env/lib/python3.12/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n",
      "    except TypeError as e:\n",
      "tensorflow.python.framework.errors_impl.InternalError: Graph execution error:\n",
      "\n",
      "Detected at node text_preprocessor/Add defined at (most recent call last):\n",
      "<stack traces unavailable>\n",
      "Detected at node text_preprocessor/Add defined at (most recent call last):\n",
      "<stack traces unavailable>\n",
      "2 root error(s) found.\n",
      "  (0) INTERNAL:  'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n",
      "\t [[{{node text_preprocessor/Add}}]]\n",
      "\t [[StatefulPartitionedCall/StatefulPartitionedCall/text_preprocessor/Where/_40]]\n",
      "  (1) INTERNAL:  'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n",
      "\t [[{{node text_preprocessor/Add}}]]\n",
      "0 successful operations.\n",
      "0 derived errors ignored. [Op:__inference_restored_function_body_16816]\n",
      "[Succeeded / Failed / Skipped / Total] 0 / 0 / 2 / 2:   2%|▏         | 2/100 [00:04<03:23,  2.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 5. (可选) 手动运行单个实验\n",
    "#\n",
    "# 您可以复制下面的单元格，修改 `attack_method`、`defense_method` 等参数，然后单独运行该单元格来测试特定的配置。\n",
    "\n",
    "# %%\n",
    "# --- 手动运行示例 ---\n",
    "run_single_experiment(\n",
    "    mode='attack',\n",
    "    attack_method='bertattack', # <--- 修改这里\n",
    "    defense_method='none',       # <--- 修改这里\n",
    "    dataset_name='agnews',        # <--- 修改这里\n",
    "    #num_examples=10,            # <--- 使用少量样本测试\n",
    "    **common_params\n",
    ")\n",
    "# # 清理显存\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13e9063b-de35-412b-be8c-0ef201bf52bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>defense</th>\n",
       "      <th>attack</th>\n",
       "      <th>num_examples</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>attack_success_rate</th>\n",
       "      <th>avg_perturbed_words</th>\n",
       "      <th>avg_queries</th>\n",
       "      <th>query_budget</th>\n",
       "      <th>mask_rate</th>\n",
       "      <th>ahp_pruning</th>\n",
       "      <th>ahp_aggregation</th>\n",
       "      <th>denoiser</th>\n",
       "      <th>ensemble_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>agnews</td>\n",
       "      <td>alpaca-native</td>\n",
       "      <td>none</td>\n",
       "      <td>deepwordbug</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>61.5000</td>\n",
       "      <td>200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sst2</td>\n",
       "      <td>alpaca-native</td>\n",
       "      <td>none</td>\n",
       "      <td>deepwordbug</td>\n",
       "      <td>100</td>\n",
       "      <td>0.6098</td>\n",
       "      <td>0.3902</td>\n",
       "      <td>1.2500</td>\n",
       "      <td>42.1220</td>\n",
       "      <td>200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sst2</td>\n",
       "      <td>alpaca-native</td>\n",
       "      <td>ahp</td>\n",
       "      <td>deepwordbug</td>\n",
       "      <td>100</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>45.9722</td>\n",
       "      <td>200</td>\n",
       "      <td>0.15</td>\n",
       "      <td>semantic</td>\n",
       "      <td>majority_vote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>agnews</td>\n",
       "      <td>alpaca-native</td>\n",
       "      <td>ahp</td>\n",
       "      <td>deepwordbug</td>\n",
       "      <td>100</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>76.4400</td>\n",
       "      <td>200</td>\n",
       "      <td>0.15</td>\n",
       "      <td>semantic</td>\n",
       "      <td>majority_vote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>agnews</td>\n",
       "      <td>alpaca-native</td>\n",
       "      <td>ahp</td>\n",
       "      <td>deepwordbug</td>\n",
       "      <td>100</td>\n",
       "      <td>0.2800</td>\n",
       "      <td>0.7200</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>77.8000</td>\n",
       "      <td>200</td>\n",
       "      <td>0.15</td>\n",
       "      <td>none</td>\n",
       "      <td>majority_vote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sst2</td>\n",
       "      <td>alpaca-native</td>\n",
       "      <td>ahp</td>\n",
       "      <td>deepwordbug</td>\n",
       "      <td>100</td>\n",
       "      <td>0.8108</td>\n",
       "      <td>0.1892</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>47.2432</td>\n",
       "      <td>200</td>\n",
       "      <td>0.15</td>\n",
       "      <td>none</td>\n",
       "      <td>majority_vote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sst2</td>\n",
       "      <td>alpaca-native</td>\n",
       "      <td>ahp</td>\n",
       "      <td>deepwordbug</td>\n",
       "      <td>100</td>\n",
       "      <td>0.8108</td>\n",
       "      <td>0.1892</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>47.2432</td>\n",
       "      <td>200</td>\n",
       "      <td>0.15</td>\n",
       "      <td>none</td>\n",
       "      <td>majority_vote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sst2</td>\n",
       "      <td>alpaca-native</td>\n",
       "      <td>selfdenoise</td>\n",
       "      <td>deepwordbug</td>\n",
       "      <td>100</td>\n",
       "      <td>0.8974</td>\n",
       "      <td>0.1026</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>47.1795</td>\n",
       "      <td>200</td>\n",
       "      <td>0.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>alpaca</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sst2</td>\n",
       "      <td>alpaca-native</td>\n",
       "      <td>selfdenoise</td>\n",
       "      <td>deepwordbug</td>\n",
       "      <td>100</td>\n",
       "      <td>0.8947</td>\n",
       "      <td>0.1053</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>47.1579</td>\n",
       "      <td>200</td>\n",
       "      <td>0.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>alpaca</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sst2</td>\n",
       "      <td>alpaca-native</td>\n",
       "      <td>none</td>\n",
       "      <td>pwws</td>\n",
       "      <td>100</td>\n",
       "      <td>0.5854</td>\n",
       "      <td>0.4146</td>\n",
       "      <td>1.1176</td>\n",
       "      <td>128.6585</td>\n",
       "      <td>200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>agnews</td>\n",
       "      <td>alpaca-native</td>\n",
       "      <td>none</td>\n",
       "      <td>pwws</td>\n",
       "      <td>100</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>1.7500</td>\n",
       "      <td>192.2917</td>\n",
       "      <td>200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dataset     model        defense       attack     num_examples  accuracy  attack_success_rate  avg_perturbed_words  avg_queries  query_budget  mask_rate ahp_pruning ahp_aggregation denoiser  ensemble_size\n",
       "0   agnews  alpaca-native         none  deepwordbug       100       0.1250         0.8750               2.0000           61.5000         200         NaN           NaN              NaN     NaN        NaN     \n",
       "1     sst2  alpaca-native         none  deepwordbug       100       0.6098         0.3902               1.2500           42.1220         200         NaN           NaN              NaN     NaN        NaN     \n",
       "2     sst2  alpaca-native          ahp  deepwordbug       100       0.7500         0.2500               1.0000           45.9722         200        0.15      semantic    majority_vote     NaN        NaN     \n",
       "3   agnews  alpaca-native          ahp  deepwordbug       100       0.2000         0.8000               1.0000           76.4400         200        0.15      semantic    majority_vote     NaN        NaN     \n",
       "4   agnews  alpaca-native          ahp  deepwordbug       100       0.2800         0.7200               1.0000           77.8000         200        0.15          none    majority_vote     NaN        NaN     \n",
       "5     sst2  alpaca-native          ahp  deepwordbug       100       0.8108         0.1892               1.0000           47.2432         200        0.15          none    majority_vote     NaN        NaN     \n",
       "6     sst2  alpaca-native          ahp  deepwordbug       100       0.8108         0.1892               1.0000           47.2432         200        0.15          none    majority_vote     NaN        NaN     \n",
       "7     sst2  alpaca-native  selfdenoise  deepwordbug       100       0.8974         0.1026               1.0000           47.1795         200        0.15           NaN              NaN  alpaca       30.0     \n",
       "8     sst2  alpaca-native  selfdenoise  deepwordbug       100       0.8947         0.1053               1.0000           47.1579         200        0.15           NaN              NaN  alpaca       10.0     \n",
       "9     sst2  alpaca-native         none         pwws       100       0.5854         0.4146               1.1176          128.6585         200         NaN           NaN              NaN     NaN        NaN     \n",
       "10  agnews  alpaca-native         none         pwws       100       0.8333         0.1667               1.7500          192.2917         200         NaN           NaN              NaN     NaN        NaN     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 6. 加载并显示结果\n",
    "#\n",
    "# 加载 `results_file` 中汇总的所有实验结果。\n",
    "\n",
    "# %%\n",
    "results_filepath = common_params[\"results_file\"]\n",
    "try:\n",
    "    df_results = pd.read_csv(results_filepath)\n",
    "    logging.info(f\"成功从 {results_filepath} 加载结果。\")\n",
    "    # 设置 Pandas 显示选项以查看更多内容\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    pd.set_option('display.colheader_justify', 'center')\n",
    "    pd.set_option('display.precision', 4) # 显示 4 位小数\n",
    "\n",
    "    # 显示结果 DataFrame\n",
    "    display(df_results)\n",
    "\n",
    "    # 可以进一步对结果进行排序或筛选\n",
    "    # print(\"\\n按攻击成功率降序排序:\")\n",
    "    # display(df_results.sort_values(by='attack_success_rate', ascending=False))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    logging.error(f\"结果文件 {results_filepath} 未找到。请先运行实验。\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"加载或显示结果时出错: {e}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# --- Notebook 结束 ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "291103f5-1b6d-42c8-84ff-1fe4828b6792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists(results_filepath):\n",
    "#     logging.warning(f\"正在删除旧的结果文件: {results_filepath}\")\n",
    "#     os.remove(results_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b1b28af-46da-4be9-946c-bf1e444826b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# # 假设您的 notebook 在 'notebooks/' 目录下，添加 'src'\n",
    "# # (如果路径不对，请调整)\n",
    "# sys.path.append('../') \n",
    "\n",
    "# import torch\n",
    "# import logging\n",
    "# from src.args_config import AHPSettings\n",
    "# from src.models.model_loader import AlpacaModel\n",
    "# from src.components.masking import AdversarialMasker\n",
    "# from src.components.candidate_generation import CandidateGenerator\n",
    "\n",
    "# # --- 1. 设置 (使用 AG News) ---\n",
    "# args = AHPSettings().parse_args([\n",
    "#     '--dataset_name', 'agnews',\n",
    "#     '--model_path', '/root/autodl-tmp/alpaca-native', # <--- 使用您原始的模型路径\n",
    "#     '--cache_dir', '/root/autodl-tmp/cache_path',\n",
    "#     '--ahp_num_candidates', '5' # 只生成 5 个用于测试\n",
    "# ])\n",
    "\n",
    "# # --- 2. 加载模型 (这会比较慢，因为要 resize token) ---\n",
    "# print(\"正在加载 AlpacaModel...\")\n",
    "# # (确保您的 AutoDL 代理和离线环境变量已设置)\n",
    "# model_wrapper = AlpacaModel(args)\n",
    "# print(\"模型加载完成。\")\n",
    "\n",
    "# # --- 3. 手动初始化 AHP 组件 ---\n",
    "# # (这模仿了 _apply_ahp_defense 中的逻辑)\n",
    "# adversarial_masker = AdversarialMasker(model_wrapper)\n",
    "# candidate_generator = CandidateGenerator(model_wrapper) #\n",
    "\n",
    "# # --- 4. 准备一个 AG News 样本 ---\n",
    "# # (一个 \"Business\" 类的样本)\n",
    "# test_text = \"Title: Wall St. Bears Claw Back Into the Black (Reuters) Description: Reuters - Short-sellers, Wall Street's dwindling band of ultra-cynics, are seeing green again.\"\n",
    "# test_label = 2 # (Business)\n",
    "\n",
    "# print(\"=\"*50)\n",
    "# print(f\"原始文本 (标签 {test_label}):\\n{test_text}\")\n",
    "# print(\"=\"*50)\n",
    "\n",
    "# # --- 5. 步骤 1：对抗性遮蔽 ---\n",
    "# print(\"正在运行对抗性遮蔽 (梯度显著图)...\")\n",
    "# masked_text, masked_indices = adversarial_masker.mask_input(test_text, args.mask_rate)\n",
    "# print(f\"遮蔽后的文本:\\n{masked_text}\")\n",
    "# print(f\"被遮蔽的索引: {masked_indices}\")\n",
    "# print(\"=\"*50)\n",
    "\n",
    "# # --- 6. 步骤 2：候选生成 ---\n",
    "# print(f\"正在生成 {args.ahp_num_candidates} 个候选句...\")\n",
    "# candidates = candidate_generator.generate_candidates(masked_text)\n",
    "\n",
    "# print(f\"--- 生成的候选句 ---\")\n",
    "# for i, cand in enumerate(candidates):\n",
    "#     print(f\"候选 {i+1}: {cand}\\n\")\n",
    "# print(\"=\"*50)\n",
    "\n",
    "# # --- 7. (可选) 检查原始预测 ---\n",
    "# print(\"正在检查模型对原始文本的预测...\")\n",
    "# original_prob = model_wrapper.predict_batch([test_text])[0]\n",
    "# print(f\"原始预测概率: {original_prob}\")\n",
    "# print(f\"原始预测类别: {original_prob.argmax()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e6c543-dbf5-4a5e-abce-7644e41090de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9abface6-9bf9-4101-a7b7-1d737184649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# import ssl\n",
    "# import logging\n",
    "\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     # 忽略 Python 2.7.9 以下的版本 (不太可能)\n",
    "#     pass\n",
    "# else:\n",
    "#     # 猴子补丁 (Monkey-patching)：\n",
    "#     # 告诉 Python 的 ssl 模块使用一个不验证证书的上下文作为默认\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "#     logging.info(\"已临时禁用 SSL 证书验证 (用于 NLTK 下载)。\")\n",
    "\n",
    "# # 下载 PWWS 和 TextBugger 可能需要的所有 NLTK 数据包\n",
    "# packages_to_download = ['wordnet', 'omw-1.4', 'averaged_perceptron_tagger', 'punkt', 'stopwords']\n",
    "# logging.info(f\"正在下载 NLTK 数据包: {packages_to_download}\")\n",
    "\n",
    "# for package in packages_to_download:\n",
    "#     try:\n",
    "#         nltk.download(package)\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"下载 {package} 时出错: {e}\")\n",
    "\n",
    "# logging.info(\"NLTK 数据包下载（或检查）完成。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AHP-Env)",
   "language": "python",
   "name": "ahp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
