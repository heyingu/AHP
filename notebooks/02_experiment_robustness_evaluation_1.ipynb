{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a746950b-94a3-4474-9210-d1d1cf1d788e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 补丁1生效：已成功注入“全功能”伪模块。\n",
      ">>> 补丁2生效：已成功替换 nltk.download 函数。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 21:24:07.098890: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-21 21:24:07.153979: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-21 21:24:08.361612: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前使用的设备: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/ahp_env/lib/python3.12/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from unittest.mock import patch\n",
    "import types\n",
    "from importlib.machinery import ModuleSpec\n",
    "\n",
    "\n",
    "# --- 补丁1：注入“全功能”伪模块 ---\n",
    "def create_full_fake_module(name, attributes_to_add):\n",
    "    spec = ModuleSpec(name, None)\n",
    "    module = types.ModuleType(name)\n",
    "    module.__spec__ = spec\n",
    "    for attr in attributes_to_add:\n",
    "        setattr(module, attr, lambda *args, **kwargs: None)\n",
    "    return module\n",
    "\n",
    "sys.modules['num2words'] = create_full_fake_module('num2words', ['num2words'])\n",
    "sys.modules['word2number'] = create_full_fake_module('word2number', ['w2n'])\n",
    "print(\">>> 补丁1生效：已成功注入“全功能”伪模块。\")\n",
    "\n",
    "# --- 补丁2：“直捣黄龙”，直接替换nltk.download函数 ---\n",
    "def dummy_nltk_download(*args, **kwargs):\n",
    "    print(\">>> 补丁2生效：已成功拦截并跳过 nltk.download() 调用！<<<\")\n",
    "    return True # 返回成功状态\n",
    "\n",
    "# 使用正确的函数路径进行替换\n",
    "patcher = patch('nltk.download', dummy_nltk_download)\n",
    "patcher.start()\n",
    "print(\">>> 补丁2生效：已成功替换 nltk.download 函数。\")\n",
    "\n",
    "\n",
    "# --- 补丁3：手动为NLTK“指路” ---\n",
    "nltk_data_dir = os.path.expanduser('~/nltk_data')\n",
    "import nltk\n",
    "if nltk_data_dir not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_dir)\n",
    "    print(f\"成功将 '{nltk_data_dir}' 添加到NLTK的搜索路径。\")\n",
    "\n",
    "# --- 补丁4：设置其他环境变量 ---\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false' \n",
    "sys.path.append('..')\n",
    "# ====================================================================\n",
    "from src.models.model_loader import load_main_llm\n",
    "from src.utils.data_loader import load_sst2_dataset, load_agnews_dataset\n",
    "from src.pipeline import AhpPipeline\n",
    "from src.defenses import baseline_defense # 引入一个简单的基线模型\n",
    "from textattack.models.wrappers import ModelWrapper\n",
    "from textattack.attack_recipes import TextBuggerLi2018, DeepWordBugGao2018\n",
    "from textattack import Attacker\n",
    "from textattack.datasets import HuggingFaceDataset,Dataset\n",
    "\n",
    "# 确保PyTorch能使用GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"当前使用的设备: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7cfc55f-c7fb-48d9-a94d-08e39a3ef190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准备从本地路径加载主模型: /root/autodl-tmp/circulus_alpaca-7b\n",
      "正在加载主模型: /root/autodl-tmp/circulus_alpaca-7b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['pad_token_id']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25016146e10a43aeb20af51585ef14b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主模型加载成功。\n"
     ]
    }
   ],
   "source": [
    "# --- 在这里选择要测试的数据集 ---\n",
    "# 可选项: 'sst2' 或 'ag_news'\n",
    "TASK_NAME = 'ag_news' \n",
    "# TASK_NAME = 'sst2'\n",
    "\n",
    "# 为了快速得到结果，我们先在少量样本上测试\n",
    "NUM_SAMPLES_TO_TEST = 50 \n",
    "\n",
    "# 加载主模型 ( circulus/alpaca-7b )\n",
    "local_model_path = \"/root/autodl-tmp/circulus_alpaca-7b\"\n",
    "print(f\"准备从本地路径加载主模型: {local_model_path}\")\n",
    "main_model, main_tokenizer = load_main_llm(model_name=local_model_path, use_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74c455a9-6a04-4000-a056-536bb0096907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载AG News数据集 (test split)...\n",
      "AG News数据集加载成功。\n",
      "\n",
      "已通过本地方式准备好攻击数据集: ag_news, 样本数: 50\n"
     ]
    }
   ],
   "source": [
    "if TASK_NAME == 'sst2':\n",
    "    dataset_hf = load_sst2_dataset(split='validation')\n",
    "    label_map = {\"negative\": 0, \"positive\": 1}\n",
    "    text_column = \"sentence\"\n",
    "    label_column = \"label_text\"\n",
    "\n",
    "elif TASK_NAME == 'ag_news':\n",
    "    dataset_hf = load_agnews_dataset(split='test')\n",
    "    label_map = {\"World\": 0, \"Sports\": 1, \"Business\": 2, \"Sci/Tech\": 3}\n",
    "    text_column = \"text\"\n",
    "    label_column = \"label_text\"\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"未知的任务名称: {TASK_NAME}\")\n",
    "\n",
    "# --- 核心修改：从内存中的数据手动构建 TextAttack 数据集 ---\n",
    "\n",
    "# 1. 从 Hugging Face 数据集中提取文本和标签，并转换为 (文本, 标签ID) 的元组列表\n",
    "#    这是我们需要的本地数据格式\n",
    "local_data_samples = [\n",
    "    (row[text_column], label_map[row[label_column]]) \n",
    "    for row in dataset_hf.shuffle().select(range(NUM_SAMPLES_TO_TEST))\n",
    "]\n",
    "\n",
    "# 2. 使用通用的 textattack.Dataset 类来封装我们的本地数据\n",
    "attack_dataset = Dataset(local_data_samples)\n",
    "\n",
    "print(f\"\\n已通过本地方式准备好攻击数据集: {TASK_NAME}, 样本数: {len(local_data_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d2df8db-3917-4662-9a3e-34effa0b4c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 单元格 4: 定义 TextAttack 模型包装器 (最终修正版)\n",
    "# ===================================================================\n",
    "import numpy as np\n",
    "\n",
    "# TextAttack 需要一个包装器来调用我们的模型或防御框架\n",
    "class DefenseModelWrapper(ModelWrapper):\n",
    "    def __init__(self, defense_pipeline):\n",
    "        self.model = defense_pipeline\n",
    "        self.num_labels = len(label_map)\n",
    "    \n",
    "    def __call__(self, text_input_list):\n",
    "        # 1. 正常获取模型的预测结果 (数字ID列表)\n",
    "        predictions = [self.model.predict_single(text) for text in text_input_list]\n",
    "        pred_ids = [label_map.get(pred_text, 0) for pred_text in predictions]\n",
    "\n",
    "        # 2. 创建 one-hot 编码的向量列表\n",
    "        one_hot_vectors = []\n",
    "        for pred_id in pred_ids:\n",
    "            one_hot_vector = np.zeros(self.num_labels)\n",
    "            one_hot_vector[pred_id] = 1.0\n",
    "            one_hot_vectors.append(one_hot_vector)\n",
    "            \n",
    "        # --- 核心修改：将向量列表堆叠成一个单一的Numpy数组 ---\n",
    "        # 如果输入1个句子，返回 shape (1, 4) 的数组\n",
    "        # 如果输入10个句子，返回 shape (10, 4) 的数组\n",
    "        # 这样 len(返回结果) 就等于输入的数量了\n",
    "        return np.array(one_hot_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df8ac275-ec6a-4052-b4c4-ad6e9f3326c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在初始化所有防御方法...\n",
      "正在加载NLI模型: roberta-large-mnli...\n",
      "NLI模型加载成功。\n",
      "防御方法初始化完成。\n"
     ]
    }
   ],
   "source": [
    "print(\"正在初始化所有防御方法...\")\n",
    "\n",
    "# 1. 无防御的原始模型\n",
    "baseline_model = baseline_defense(main_model, main_tokenizer, task=TASK_NAME)\n",
    "baseline_wrapper = DefenseModelWrapper(baseline_model)\n",
    "\n",
    "# 2. AHP 防御框架 (使用实验一选出的最优规则 'nli')\n",
    "ahp_defense = AhpPipeline(\n",
    "    main_model=main_model,\n",
    "    main_tokenizer=main_tokenizer,\n",
    "    pruner_name='nli', # 使用最优规则\n",
    "    k_val=3,\n",
    "    m_val=10,\n",
    "    task=TASK_NAME\n",
    ")\n",
    "ahp_wrapper = DefenseModelWrapper(ahp_defense)\n",
    "\n",
    "# 将所有防御方法放入字典，方便遍历\n",
    "defenses_to_test = {\n",
    "    \"Baseline (No Defense)\": baseline_wrapper,\n",
    "    \"AHP Defense (nli-pruner)\": ahp_wrapper,\n",
    "    # TODO: 未来可以添加 Self-Denoise 等其他防御方法\n",
    "}\n",
    "\n",
    "print(\"防御方法初始化完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "050eb821-874d-4cda-82a3-e943dfe0a012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在初始化攻击方法...\n",
      "攻击方法初始化完成。\n"
     ]
    }
   ],
   "source": [
    "print(\"正在初始化攻击方法...\")\n",
    "\n",
    "attacks_to_test = {\n",
    "    \"TextBugger\": TextBuggerLi2018,\n",
    "    \"DeepWordBug\": DeepWordBugGao2018,\n",
    "}\n",
    "\n",
    "print(\"攻击方法初始化完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25eaa80e-b276-43cd-b0e6-0dcedd78292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for defense_name, defense_wrapper in defenses_to_test.items():\n",
    "    for attack_name, attack_recipe in attacks_to_test.items():\n",
    "        print(f\"\\n{'='*20} 正在评估 {'='*20}\")\n",
    "        print(f\"  防御方法: {defense_name}\")\n",
    "        print(f\"  攻击算法: {attack_name}\")\n",
    "        print(f\"{'='*46}\")\n",
    "\n",
    "        # 配置攻击\n",
    "        attacker = Attacker(attack_recipe.build(defense_wrapper), attack_dataset)\n",
    "        \n",
    "        # 执行攻击并获取结果\n",
    "        attack_results = attacker.attack_dataset()\n",
    "\n",
    "        # 从结果中提取我们需要的指标\n",
    "        num_total = len(attack_results)\n",
    "        num_failures = sum(1 for r in attack_results if r.goal_function_result.succeeded)\n",
    "        num_successes = num_total - num_failures\n",
    "        \n",
    "        # 原始准确率 (在攻击成功+失败的样本上的准确率)\n",
    "        original_accuracy = (num_successes / num_total) * 100 if num_total > 0 else 0\n",
    "        \n",
    "        # 攻击后准确率 (只计算攻击失败的样本，因为成功的都被攻击器改变了标签)\n",
    "        # 注意: TextAttack的结果对象里，num_failures代表攻击成功，num_successes代表攻击失败\n",
    "        accuracy_under_attack = (sum(1 for r in attack_results if not r.goal_function_result.succeeded) / num_total) * 100 if num_total > 0 else 0\n",
    "        \n",
    "        # 攻击成功率 (ASR)\n",
    "        # 在原始预测正确的样本中，有多少被成功攻击了\n",
    "        num_originally_correct = sum(1 for r in attack_results if r.original_result.goal_function_result.succeeded)\n",
    "        num_attack_success = sum(1 for r in attack_results if r.goal_function_result.succeeded)\n",
    "        attack_success_rate = (num_attack_success / num_originally_correct) * 100 if num_originally_correct > 0 else 0\n",
    "\n",
    "        print(\"\\n评估完成:\")\n",
    "        print(f\"  - 原始准确率 (Clean Accuracy): {original_accuracy:.2f}%\")\n",
    "        print(f\"  - 攻击后准确率 (Accuracy under Attack): {accuracy_under_attack:.2f}%\")\n",
    "        print(f\"  - 攻击成功率 (ASR): {attack_success_rate:.2f}%\")\n",
    "\n",
    "        results.append({\n",
    "            \"Defense\": defense_name,\n",
    "            \"Attack\": attack_name,\n",
    "            \"Clean Accuracy (%)\": original_accuracy,\n",
    "            \"Accuracy under Attack (%)\": accuracy_under_attack,\n",
    "            \"Attack Success Rate (%)\": attack_success_rate\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3556f79d-197d-4e55-8425-6c6a95ca5498",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*30)\n",
    "print(\"实验二：鲁棒性评估 - 结果汇总\")\n",
    "print(\"=\"*30)\n",
    "print(results_df.to_string())\n",
    "\n",
    "# --- 自动编号并保存结果 ---\n",
    "if not os.path.exists('../results'):\n",
    "    os.makedirs('../results')\n",
    "\n",
    "base_path = f'../results/experiment_2_robustness_{TASK_NAME}'\n",
    "extension = '.csv'\n",
    "save_path = f\"{base_path}{extension}\"\n",
    "\n",
    "counter = 1\n",
    "while os.path.exists(save_path):\n",
    "    save_path = f\"{base_path}_{counter}{extension}\"\n",
    "    counter += 1\n",
    "\n",
    "results_df.to_csv(save_path, index=False)\n",
    "print(f\"\\n实验结果已成功保存到: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AHP-Env)",
   "language": "python",
   "name": "ahp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
