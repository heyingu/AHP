{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a746950b-94a3-4474-9210-d1d1cf1d788e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 补丁1生效：已成功注入“全功能”伪模块。\n",
      ">>> 补丁2生效：已成功替换 nltk.download 函数。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 14:08:52.291007: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-23 14:08:52.568093: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-23 14:08:54.030475: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/root/ahp_env/lib/python3.12/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前使用的设备: cuda\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from unittest.mock import patch\n",
    "import types\n",
    "from importlib.machinery import ModuleSpec\n",
    "\n",
    "\n",
    "# --- 补丁1：注入“全功能”伪模块 ---\n",
    "def create_full_fake_module(name, attributes_to_add):\n",
    "    spec = ModuleSpec(name, None)\n",
    "    module = types.ModuleType(name)\n",
    "    module.__spec__ = spec\n",
    "    for attr in attributes_to_add:\n",
    "        setattr(module, attr, lambda *args, **kwargs: None)\n",
    "    return module\n",
    "\n",
    "sys.modules['num2words'] = create_full_fake_module('num2words', ['num2words'])\n",
    "sys.modules['word2number'] = create_full_fake_module('word2number', ['w2n'])\n",
    "print(\">>> 补丁1生效：已成功注入“全功能”伪模块。\")\n",
    "\n",
    "# --- 补丁2：“直捣黄龙”，直接替换nltk.download函数 ---\n",
    "def dummy_nltk_download(*args, **kwargs):\n",
    "    print(\">>> 补丁2生效：已成功拦截并跳过 nltk.download() 调用！<<<\")\n",
    "    return True # 返回成功状态\n",
    "\n",
    "# 使用正确的函数路径进行替换\n",
    "patcher = patch('nltk.download', dummy_nltk_download)\n",
    "patcher.start()\n",
    "print(\">>> 补丁2生效：已成功替换 nltk.download 函数。\")\n",
    "\n",
    "\n",
    "# --- 补丁3：手动为NLTK“指路” ---\n",
    "nltk_data_dir = os.path.expanduser('~/nltk_data')\n",
    "import nltk\n",
    "if nltk_data_dir not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_dir)\n",
    "    print(f\"成功将 '{nltk_data_dir}' 添加到NLTK的搜索路径。\")\n",
    "\n",
    "# --- 补丁4：设置其他环境变量 ---\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false' \n",
    "sys.path.append('..')\n",
    "# ====================================================================\n",
    "from src.models.model_loader import load_main_llm\n",
    "from src.utils.data_loader import load_sst2_dataset, load_agnews_dataset\n",
    "from src.pipeline import AhpPipeline\n",
    "from src.defenses import baseline_defense # 引入一个简单的基线模型\n",
    "from textattack.models.wrappers import ModelWrapper\n",
    "from textattack.attack_recipes import TextBuggerLi2018, DeepWordBugGao2018\n",
    "from textattack import Attacker\n",
    "from textattack.datasets import HuggingFaceDataset,Dataset\n",
    "\n",
    "# 确保PyTorch能使用GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"当前使用的设备: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7cfc55f-c7fb-48d9-a94d-08e39a3ef190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准备从本地路径加载主模型: /root/autodl-tmp/circulus_alpaca-7b\n",
      "正在加载主模型: /root/autodl-tmp/circulus_alpaca-7b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d0aa60d0cc4f3a9d8289ad18ce84eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "主模型加载成功。\n"
     ]
    }
   ],
   "source": [
    "# --- 在这里选择要测试的数据集 ---\n",
    "# 可选项: 'sst2' 或 'ag_news'\n",
    "# TASK_NAME = 'ag_news' \n",
    "TASK_NAME = 'sst2'\n",
    "\n",
    "# 为了快速得到结果，我们先在少量样本上测试\n",
    "NUM_SAMPLES_TO_TEST = 50 \n",
    "\n",
    "# 加载主模型 ( circulus/alpaca-7b )\n",
    "local_model_path = \"/root/autodl-tmp/circulus_alpaca-7b\"\n",
    "print(f\"准备从本地路径加载主模型: {local_model_path}\")\n",
    "main_model, main_tokenizer = load_main_llm(model_name=local_model_path, use_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74c455a9-6a04-4000-a056-536bb0096907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载SST-2数据集 (validation split)...\n",
      "SST-2数据集加载成功。\n",
      "\n",
      "已通过本地方式准备好攻击数据集: sst2, 样本数: 50\n"
     ]
    }
   ],
   "source": [
    "if TASK_NAME == 'sst2':\n",
    "    dataset_hf = load_sst2_dataset(split='validation')\n",
    "    label_map = {\"negative\": 0, \"positive\": 1}\n",
    "    text_column = \"sentence\"\n",
    "    label_column = \"label_text\"\n",
    "\n",
    "elif TASK_NAME == 'ag_news':\n",
    "    dataset_hf = load_agnews_dataset(split='test')\n",
    "    label_map = {\"World\": 0, \"Sports\": 1, \"Business\": 2, \"Sci/Tech\": 3}\n",
    "    text_column = \"text\"\n",
    "    label_column = \"label_text\"\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"未知的任务名称: {TASK_NAME}\")\n",
    "\n",
    "# --- 核心修改：从内存中的数据手动构建 TextAttack 数据集 ---\n",
    "\n",
    "# 1. 从 Hugging Face 数据集中提取文本和标签，并转换为 (文本, 标签ID) 的元组列表\n",
    "#    这是我们需要的本地数据格式\n",
    "local_data_samples = [\n",
    "    (row[text_column], label_map[row[label_column]]) \n",
    "    for row in dataset_hf.shuffle().select(range(NUM_SAMPLES_TO_TEST))\n",
    "]\n",
    "\n",
    "# 2. 使用通用的 textattack.Dataset 类来封装我们的本地数据\n",
    "attack_dataset = Dataset(local_data_samples)\n",
    "\n",
    "print(f\"\\n已通过本地方式准备好攻击数据集: {TASK_NAME}, 样本数: {len(local_data_samples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d2df8db-3917-4662-9a3e-34effa0b4c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 单元格 4: 定义 TextAttack 模型包装器 (最终修正版)\n",
    "# ===================================================================\n",
    "import numpy as np\n",
    "\n",
    "# TextAttack 需要一个包装器来调用我们的模型或防御框架\n",
    "class DefenseModelWrapper(ModelWrapper):\n",
    "    def __init__(self, defense_pipeline):\n",
    "        self.model = defense_pipeline\n",
    "        self.num_labels = len(label_map)\n",
    "    \n",
    "    def __call__(self, text_input_list):\n",
    "        # 1. 正常获取模型的预测结果 (数字ID列表)\n",
    "        predictions = [self.model.predict_single(text) for text in text_input_list]\n",
    "        pred_ids = [label_map.get(pred_text, 0) for pred_text in predictions]\n",
    "\n",
    "        # 2. 创建 one-hot 编码的向量列表\n",
    "        one_hot_vectors = []\n",
    "        for pred_id in pred_ids:\n",
    "            one_hot_vector = np.zeros(self.num_labels)\n",
    "            one_hot_vector[pred_id] = 1.0\n",
    "            one_hot_vectors.append(one_hot_vector)\n",
    "            \n",
    "        # --- 核心修改：将向量列表堆叠成一个单一的Numpy数组 ---\n",
    "        # 如果输入1个句子，返回 shape (1, 4) 的数组\n",
    "        # 如果输入10个句子，返回 shape (10, 4) 的数组\n",
    "        # 这样 len(返回结果) 就等于输入的数量了\n",
    "        return np.array(one_hot_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df8ac275-ec6a-4052-b4c4-ad6e9f3326c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在初始化所有防御方法...\n",
      "正在加载NLI模型: roberta-large-mnli...\n",
      "NLI模型加载成功。\n",
      "防御方法初始化完成。\n"
     ]
    }
   ],
   "source": [
    "print(\"正在初始化所有防御方法...\")\n",
    "\n",
    "# 1. 无防御的原始模型\n",
    "baseline_model = baseline_defense(main_model, main_tokenizer, task=TASK_NAME)\n",
    "baseline_wrapper = DefenseModelWrapper(baseline_model)\n",
    "\n",
    "# 2. AHP 防御框架 (使用实验一选出的最优规则 'nli')\n",
    "ahp_defense = AhpPipeline(\n",
    "    main_model=main_model,\n",
    "    main_tokenizer=main_tokenizer,\n",
    "    pruner_name='nli', # 使用最优规则\n",
    "    k_val=3,\n",
    "    m_val=10,\n",
    "    task=TASK_NAME\n",
    ")\n",
    "ahp_wrapper = DefenseModelWrapper(ahp_defense)\n",
    "\n",
    "# 将所有防御方法放入字典，方便遍历\n",
    "defenses_to_test = {\n",
    "    \"Baseline (No Defense)\": baseline_wrapper,\n",
    "    \"AHP Defense (nli-pruner)\": ahp_wrapper,\n",
    "    # TODO: 未来可以添加 Self-Denoise 等其他防御方法\n",
    "}\n",
    "\n",
    "print(\"防御方法初始化完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "050eb821-874d-4cda-82a3-e943dfe0a012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在初始化攻击方法...\n",
      "攻击方法初始化完成。\n"
     ]
    }
   ],
   "source": [
    "print(\"正在初始化攻击方法...\")\n",
    "\n",
    "attacks_to_test = {\n",
    "    \"TextBugger\": TextBuggerLi2018,\n",
    "    \"DeepWordBug\": DeepWordBugGao2018,\n",
    "}\n",
    "\n",
    "print(\"攻击方法初始化完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25eaa80e-b276-43cd-b0e6-0dcedd78292d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== 正在评估 ====================\n",
      "  防御方法: Baseline (No Defense)\n",
      "  攻击算法: TextBugger\n",
      "==============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'src.defenses.baseline_defense'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedyWordSwapWIR(\n",
      "    (wir_method):  delete\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  CompositeTransformation(\n",
      "    (0): WordSwapRandomCharacterInsertion(\n",
      "        (random_one):  True\n",
      "      )\n",
      "    (1): WordSwapRandomCharacterDeletion(\n",
      "        (random_one):  True\n",
      "      )\n",
      "    (2): WordSwapNeighboringCharacterSwap(\n",
      "        (random_one):  True\n",
      "      )\n",
      "    (3): WordSwapHomoglyphSwap\n",
      "    (4): WordSwapEmbedding(\n",
      "        (max_candidates):  5\n",
      "        (embedding):  WordEmbedding\n",
      "      )\n",
      "    )\n",
      "  (constraints): \n",
      "    (0): UniversalSentenceEncoder(\n",
      "        (metric):  angular\n",
      "        (threshold):  0.8\n",
      "        (window_size):  inf\n",
      "        (skip_text_shorter_than_window):  False\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): RepeatModification\n",
      "    (2): StopwordModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1761200108.270678    4563 gpu_device.cc:2431] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "W0000 00:00:1761200108.295804    4563 gpu_device.cc:2431] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n",
      "I0000 00:00:1761200108.302161    4563 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 17979 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 5090, pci bus id: 0000:a8:00.0, compute capability: 12.0\n",
      "2025-10-23 14:15:18.906639: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleLoadData(&module, data)' failed with 'CUDA_ERROR_INVALID_PTX'\n",
      "\n",
      "2025-10-23 14:15:18.906674: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleGetFunction(&function, module, kernel_name)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n",
      "\n",
      "2025-10-23 14:15:18.906685: W tensorflow/core/framework/op_kernel.cc:1842] INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n",
      "2025-10-23 14:15:18.906800: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n",
      "\t [[{{node text_preprocessor/Add}}]]\n",
      "2025-10-23 14:15:18.906810: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 6315195396302004454\n",
      "2025-10-23 14:15:18.906816: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n",
      "\t [[{{node text_preprocessor/Add}}]]\n",
      "\t [[StatefulPartitionedCall/StatefulPartitionedCall/text_preprocessor/PackPositions/ones_like/_52]]\n",
      "2025-10-23 14:15:18.906821: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 1680712358278108415\n",
      "2025-10-23 14:15:18.906826: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 12383617873616022687\n",
      "2025-10-23 14:15:18.906838: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 10569587925998666318\n",
      "2025-10-23 14:15:18.906843: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 4949149369651986994\n",
      "2025-10-23 14:15:18.906846: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 9171690775637912182\n",
      "2025-10-23 14:15:18.906850: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 15993852621952798354\n",
      "2025-10-23 14:15:18.906854: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 8888342078231703306\n",
      "2025-10-23 14:15:18.906858: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 12243236930095624922\n",
      "2025-10-23 14:15:18.906862: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 6564909388550542418\n",
      "2025-10-23 14:15:18.906866: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 7854704720719149270\n",
      "2025-10-23 14:15:18.906869: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 3454021302677985362\n",
      "2025-10-23 14:15:18.906890: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 6139506526156668144\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Graph execution error:\n\nDetected at node text_preprocessor/Add defined at (most recent call last):\n<stack traces unavailable>\nDetected at node text_preprocessor/Add defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) INTERNAL:  'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n\t [[{{node text_preprocessor/Add}}]]\n\t [[StatefulPartitionedCall/StatefulPartitionedCall/text_preprocessor/PackPositions/ones_like/_52]]\n  (1) INTERNAL:  'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n\t [[{{node text_preprocessor/Add}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_restored_function_body_4204]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInternalError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m attacker = Attacker(attack_recipe.build(defense_wrapper), attack_dataset)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# 执行攻击并获取结果\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m attack_results = \u001b[43mattacker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattack_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# 从结果中提取我们需要的指标\u001b[39;00m\n\u001b[32m     17\u001b[39m num_total = \u001b[38;5;28mlen\u001b[39m(attack_results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/textattack/attacker.py:441\u001b[39m, in \u001b[36mAttacker.attack_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    439\u001b[39m     \u001b[38;5;28mself\u001b[39m._attack_parallel()\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.attack_args.silent:\n\u001b[32m    444\u001b[39m     logger.setLevel(logging.INFO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/textattack/attacker.py:170\u001b[39m, in \u001b[36mAttacker._attack\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    168\u001b[39m     result = \u001b[38;5;28mself\u001b[39m.attack.attack(example, ground_truth_output)\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    172\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(result, SkippedAttackResult) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.attack_args.attack_n\n\u001b[32m    173\u001b[39m ) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m    174\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, SuccessfulAttackResult)\n\u001b[32m    175\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.attack_args.num_successful_examples\n\u001b[32m    176\u001b[39m ):\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m worklist_candidates:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/textattack/attacker.py:168\u001b[39m, in \u001b[36mAttacker._attack\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    166\u001b[39m     example.attack_attrs[\u001b[33m\"\u001b[39m\u001b[33mlabel_names\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.dataset.label_names\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattack\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/textattack/attack.py:450\u001b[39m, in \u001b[36mAttack.attack\u001b[39m\u001b[34m(self, example, ground_truth_output)\u001b[39m\n\u001b[32m    448\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SkippedAttackResult(goal_function_result)\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m450\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgoal_function_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/textattack/attack.py:398\u001b[39m, in \u001b[36mAttack._attack\u001b[39m\u001b[34m(self, initial_result)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_attack\u001b[39m(\u001b[38;5;28mself\u001b[39m, initial_result):\n\u001b[32m    388\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Calls the ``SearchMethod`` to perturb the ``AttackedText`` stored in\u001b[39;00m\n\u001b[32m    389\u001b[39m \u001b[33;03m    ``initial_result``.\u001b[39;00m\n\u001b[32m    390\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m \u001b[33;03m            or ``MaximizedAttackResult``.\u001b[39;00m\n\u001b[32m    397\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     final_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msearch_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m     \u001b[38;5;28mself\u001b[39m.clear_cache()\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m final_result.goal_status == GoalFunctionResultStatus.SUCCEEDED:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/textattack/search_methods/search_method.py:36\u001b[39m, in \u001b[36mSearchMethod.__call__\u001b[39m\u001b[34m(self, initial_result)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfilter_transformations\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m     33\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSearch Method must have access to filter_transformations method\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     34\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mperform_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# ensure that the number of queries for this GoalFunctionResult is up-to-date\u001b[39;00m\n\u001b[32m     38\u001b[39m result.num_queries = \u001b[38;5;28mself\u001b[39m.goal_function.num_queries\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/textattack/search_methods/greedy_word_swap_wir.py:141\u001b[39m, in \u001b[36mGreedyWordSwapWIR.perform_search\u001b[39m\u001b[34m(self, initial_result)\u001b[39m\n\u001b[32m    139\u001b[39m results = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m i < \u001b[38;5;28mlen\u001b[39m(index_order) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m search_over:\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     transformed_text_candidates = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_transformations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcur_result\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattacked_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial_result\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattacked_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindices_to_modify\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex_order\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m     i += \u001b[32m1\u001b[39m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(transformed_text_candidates) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/textattack/attack.py:315\u001b[39m, in \u001b[36mAttack.get_transformations\u001b[39m\u001b[34m(self, current_text, original_text, **kwargs)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    311\u001b[39m     transformed_texts = \u001b[38;5;28mself\u001b[39m._get_transformations_uncached(\n\u001b[32m    312\u001b[39m         current_text, original_text, **kwargs\n\u001b[32m    313\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilter_transformations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformed_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_text\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/textattack/attack.py:380\u001b[39m, in \u001b[36mAttack.filter_transformations\u001b[39m\u001b[34m(self, transformed_texts, current_text, original_text)\u001b[39m\n\u001b[32m    378\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.constraints_cache[(current_text, transformed_text)]:\n\u001b[32m    379\u001b[39m             filtered_texts.append(transformed_text)\n\u001b[32m--> \u001b[39m\u001b[32m380\u001b[39m filtered_texts += \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_filter_transformations_uncached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m    \u001b[49m\u001b[43muncached_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43moriginal_text\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[38;5;66;03m# Sort transformations to ensure order is preserved between runs\u001b[39;00m\n\u001b[32m    384\u001b[39m filtered_texts.sort(key=\u001b[38;5;28;01mlambda\u001b[39;00m t: t.text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/textattack/attack.py:340\u001b[39m, in \u001b[36mAttack._filter_transformations_uncached\u001b[39m\u001b[34m(self, transformed_texts, current_text, original_text)\u001b[39m\n\u001b[32m    335\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m original_text:\n\u001b[32m    336\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    337\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing `original_text` argument when constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(C)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is set to compare against `original_text`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    338\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m     filtered_texts = \u001b[43mC\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_many\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    342\u001b[39m     filtered_texts = C.call_many(filtered_texts, current_text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/textattack/constraints/constraint.py:50\u001b[39m, in \u001b[36mConstraint.call_many\u001b[39m\u001b[34m(self, transformed_texts, reference_text)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m     48\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtransformed_text must have `last_transformation` attack_attr to apply constraint\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     49\u001b[39m         )\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m filtered_texts = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_constraint_many\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompatible_transformed_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference_text\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(filtered_texts) + incompatible_transformed_texts\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/textattack/constraints/semantics/sentence_encoders/sentence_encoder.py:179\u001b[39m, in \u001b[36mSentenceEncoder._check_constraint_many\u001b[39m\u001b[34m(self, transformed_texts, reference_text)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_constraint_many\u001b[39m(\u001b[38;5;28mself\u001b[39m, transformed_texts, reference_text):\n\u001b[32m    176\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Filters the list ``transformed_texts`` so that the similarity\u001b[39;00m\n\u001b[32m    177\u001b[39m \u001b[33;03m    between the ``reference_text`` and the transformed text is greater than\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[33;03m    the ``self.threshold``.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_score_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreference_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, transformed_text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(transformed_texts):\n\u001b[32m    182\u001b[39m         \u001b[38;5;66;03m# Optionally ignore similarity score for sentences shorter than the\u001b[39;00m\n\u001b[32m    183\u001b[39m         \u001b[38;5;66;03m# window size.\u001b[39;00m\n\u001b[32m    184\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    185\u001b[39m             \u001b[38;5;28mself\u001b[39m.skip_text_shorter_than_window\n\u001b[32m    186\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(transformed_text.words) < \u001b[38;5;28mself\u001b[39m.window_size\n\u001b[32m    187\u001b[39m         ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/textattack/constraints/semantics/sentence_encoders/sentence_encoder.py:152\u001b[39m, in \u001b[36mSentenceEncoder._score_list\u001b[39m\u001b[34m(self, starting_text, transformed_texts)\u001b[39m\n\u001b[32m    142\u001b[39m     starting_text_windows.append(\n\u001b[32m    143\u001b[39m         starting_text.text_window_around_index(\n\u001b[32m    144\u001b[39m             modified_index, \u001b[38;5;28mself\u001b[39m.window_size\n\u001b[32m    145\u001b[39m         )\n\u001b[32m    146\u001b[39m     )\n\u001b[32m    147\u001b[39m     transformed_text_windows.append(\n\u001b[32m    148\u001b[39m         transformed_text.text_window_around_index(\n\u001b[32m    149\u001b[39m             modified_index, \u001b[38;5;28mself\u001b[39m.window_size\n\u001b[32m    150\u001b[39m         )\n\u001b[32m    151\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstarting_text_windows\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_text_windows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embeddings, torch.Tensor):\n\u001b[32m    154\u001b[39m     embeddings = torch.tensor(embeddings)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/textattack/constraints/semantics/sentence_encoders/universal_sentence_encoder/universal_sentence_encoder.py:31\u001b[39m, in \u001b[36mUniversalSentenceEncoder.encode\u001b[39m\u001b[34m(self, sentences)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model:\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = hub.load(\u001b[38;5;28mself\u001b[39m._tfhub_url)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m encoding = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoding, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m     34\u001b[39m     encoding = encoding[\u001b[33m\"\u001b[39m\u001b[33moutputs\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/tensorflow/python/saved_model/load.py:817\u001b[39m, in \u001b[36m_call_attribute\u001b[39m\u001b[34m(instance, *args, **kwargs)\u001b[39m\n\u001b[32m    816\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call_attribute\u001b[39m(instance, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m817\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minstance\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    155\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ahp_env/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[32m     54\u001b[39m                                       inputs, attrs, num_outputs)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mInternalError\u001b[39m: Graph execution error:\n\nDetected at node text_preprocessor/Add defined at (most recent call last):\n<stack traces unavailable>\nDetected at node text_preprocessor/Add defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) INTERNAL:  'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n\t [[{{node text_preprocessor/Add}}]]\n\t [[StatefulPartitionedCall/StatefulPartitionedCall/text_preprocessor/PackPositions/ones_like/_52]]\n  (1) INTERNAL:  'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n\t [[{{node text_preprocessor/Add}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_restored_function_body_4204]"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for defense_name, defense_wrapper in defenses_to_test.items():\n",
    "    for attack_name, attack_recipe in attacks_to_test.items():\n",
    "        print(f\"\\n{'='*20} 正在评估 {'='*20}\")\n",
    "        print(f\"  防御方法: {defense_name}\")\n",
    "        print(f\"  攻击算法: {attack_name}\")\n",
    "        print(f\"{'='*46}\")\n",
    "\n",
    "        # 配置攻击\n",
    "        attacker = Attacker(attack_recipe.build(defense_wrapper), attack_dataset)\n",
    "        \n",
    "        # 执行攻击并获取结果\n",
    "        attack_results = attacker.attack_dataset()\n",
    "\n",
    "        # 从结果中提取我们需要的指标\n",
    "        num_total = len(attack_results)\n",
    "        num_failures = sum(1 for r in attack_results if r.goal_function_result.succeeded)\n",
    "        num_successes = num_total - num_failures\n",
    "        \n",
    "        # 原始准确率 (在攻击成功+失败的样本上的准确率)\n",
    "        original_accuracy = (num_successes / num_total) * 100 if num_total > 0 else 0\n",
    "        \n",
    "        # 攻击后准确率 (只计算攻击失败的样本，因为成功的都被攻击器改变了标签)\n",
    "        # 注意: TextAttack的结果对象里，num_failures代表攻击成功，num_successes代表攻击失败\n",
    "        accuracy_under_attack = (sum(1 for r in attack_results if not r.goal_function_result.succeeded) / num_total) * 100 if num_total > 0 else 0\n",
    "        \n",
    "        # 攻击成功率 (ASR)\n",
    "        # 在原始预测正确的样本中，有多少被成功攻击了\n",
    "        num_originally_correct = sum(1 for r in attack_results if r.original_result.goal_function_result.succeeded)\n",
    "        num_attack_success = sum(1 for r in attack_results if r.goal_function_result.succeeded)\n",
    "        attack_success_rate = (num_attack_success / num_originally_correct) * 100 if num_originally_correct > 0 else 0\n",
    "\n",
    "        print(\"\\n评估完成:\")\n",
    "        print(f\"  - 原始准确率 (Clean Accuracy): {original_accuracy:.2f}%\")\n",
    "        print(f\"  - 攻击后准确率 (Accuracy under Attack): {accuracy_under_attack:.2f}%\")\n",
    "        print(f\"  - 攻击成功率 (ASR): {attack_success_rate:.2f}%\")\n",
    "\n",
    "        results.append({\n",
    "            \"Defense\": defense_name,\n",
    "            \"Attack\": attack_name,\n",
    "            \"Clean Accuracy (%)\": original_accuracy,\n",
    "            \"Accuracy under Attack (%)\": accuracy_under_attack,\n",
    "            \"Attack Success Rate (%)\": attack_success_rate\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3556f79d-197d-4e55-8425-6c6a95ca5498",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*30)\n",
    "print(\"实验二：鲁棒性评估 - 结果汇总\")\n",
    "print(\"=\"*30)\n",
    "print(results_df.to_string())\n",
    "\n",
    "# --- 自动编号并保存结果 ---\n",
    "if not os.path.exists('../results'):\n",
    "    os.makedirs('../results')\n",
    "\n",
    "base_path = f'../results/experiment_2_robustness_{TASK_NAME}'\n",
    "extension = '.csv'\n",
    "save_path = f\"{base_path}{extension}\"\n",
    "\n",
    "counter = 1\n",
    "while os.path.exists(save_path):\n",
    "    save_path = f\"{base_path}_{counter}{extension}\"\n",
    "    counter += 1\n",
    "\n",
    "results_df.to_csv(save_path, index=False)\n",
    "print(f\"\\n实验结果已成功保存到: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AHP-Env)",
   "language": "python",
   "name": "ahp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
